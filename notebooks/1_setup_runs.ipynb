{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7df4c",
   "metadata": {},
   "source": [
    "# <font color = teal> Setup the runs </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f477ab",
   "metadata": {},
   "source": [
    "This notebook contains information about every step that you should take before starting to train or evaluate the models.\n",
    "\n",
    "1)  [<b>Data acquisition</b>](#data_acq): how to download data, with a focus on the[ Physionet 2021 dataset](#physionet) and [Shandong Provincial Hospital database](#sph)\n",
    "\n",
    "2)  [<b>Label mapping</b>](#map): how to handle the situation when the ECGs are labeled with different standards\n",
    "\n",
    "3)  [<b>Data preprocessing</b>](#data_prep): how to preprocess data before training a model, if required\n",
    "\n",
    "4)  [<b>Data splitting for training and evaluation</b>](#split): the idea of splitting data into CSV files for training, validation and testing, and how to perform it\n",
    "\n",
    "5)  [<b>Creation of the configuration files</b>](#yamls): how to create YAML files to configure the runs\n",
    "\n",
    "In the end of the notebook, an [example situation](#example) is presented to illustrate the steps mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e730",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 1) Data acquisition: PhysioNet 2021 dataset and Shandong Provincial Hospital database</font> <a id='data_acq'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13bf81a5",
   "metadata": {},
   "source": [
    "### <font color = teal> 1.1) PhysioNet Challenge 2021 </font> <a id='physionet'></a>\n",
    "\n",
    "The exploration of the dataset is available in the notebook [Exploration of the PhysioNet2021 data](exploration_physionet2021_data.ipynb).\n",
    "\n",
    "You can obtain the PhysioNet 2021 data in `tar.gz` format using either of the following methods:\n",
    "\n",
    "1) Download the data manually from [here](https://moody-challenge.physionet.org/2021/) under **Data Access**\n",
    "\n",
    "2) Utilize the provided code within this notebook to get access to the data: <font color = red> (TBA) This is not valid anymore, `wget` path and data structure changed by PhysioNet </font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21446148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os, re\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1d7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-28 14:53:34--  https://physionet.org/files/challenge-2021/1.0.3/training/\n",
      "Resolving physionet.org (physionet.org)... 18.18.42.54\n",
      "Connecting to physionet.org (physionet.org)|18.18.42.54|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]   1.15K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:35 (459 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/index.html’ saved [1177]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2024-02-28 14:53:35--  https://physionet.org/robots.txt\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n",
      "--2024-02-28 14:53:35--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]   1.44K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:35 (670 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/index.html’ saved [1471]\n",
      "\n",
      "--2024-02-28 14:53:35--  https://physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]    1023  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:35 (645 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018/index.html’ saved [1023]\n",
      "\n",
      "--2024-02-28 14:53:35--  https://physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018_extra/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018_extra/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]     711  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (420 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/cpsc_2018_extra/index.html’ saved [711]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/georgia/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/georgia/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]   1.42K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (833 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/georgia/index.html’ saved [1453]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/ningbo/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/ningbo/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]   3.97K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (706 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/ningbo/index.html’ saved [4067]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/ptb/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/ptb/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]     363  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (245 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/ptb/index.html’ saved [363]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/ptb-xl/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/ptb-xl/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]   2.59K  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (1.48 GB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/ptb-xl/index.html’ saved [2650]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/st_petersburg_incart/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/st_petersburg_incart/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]     397  --.-KB/s    in 0s      \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:36 (247 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/st_petersburg_incart/index.html’ saved [397]\n",
      "\n",
      "--2024-02-28 14:53:36--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g1/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g1/index.html’\n",
      "\n",
      "physionet.org/files     [   <=>              ] 226.73K   438KB/s    in 0.5s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:37 (438 KB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g1/index.html’ saved [232167]\n",
      "\n",
      "--2024-02-28 14:53:37--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g10/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g10/index.html’\n",
      "\n",
      "physionet.org/files     [  <=>               ] 226.95K   829KB/s    in 0.3s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:38 (829 KB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g10/index.html’ saved [232401]\n",
      "\n",
      "--2024-02-28 14:53:38--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g11/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g11/index.html’\n",
      "\n",
      "physionet.org/files     [ <=>                ]  56.58K  --.-KB/s    in 0.05s   \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:38 (1.12 MB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g11/index.html’ saved [57937]\n",
      "\n",
      "--2024-02-28 14:53:38--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g2/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g2/index.html’\n",
      "\n",
      "physionet.org/files     [  <=>               ] 226.95K   794KB/s    in 0.3s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:38 (794 KB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g2/index.html’ saved [232399]\n",
      "\n",
      "--2024-02-28 14:53:38--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g3/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g3/index.html’\n",
      "\n",
      "physionet.org/files     [  <=>               ] 226.95K   798KB/s    in 0.3s    \n",
      "\n",
      "Last-modified header missing -- time-stamps turned off.\n",
      "2024-02-28 14:53:39 (798 KB/s) - ‘physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g3/index.html’ saved [232399]\n",
      "\n",
      "--2024-02-28 14:53:39--  https://physionet.org/files/challenge-2021/1.0.3/training/chapman_shaoxing/g4/\n",
      "Reusing existing connection to physionet.org:443.\n",
      "HTTP request sent, awaiting response... "
     ]
    }
   ],
   "source": [
    "# Download the PhysioNet2021 data\n",
    "!wget -r -N -c -np https://physionet.org/files/challenge-2021/1.0.3/training/ # NEW PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0fdc",
   "metadata": {},
   "source": [
    "Once the `tar.gz` files are downloaded, they need to be extracted to the `data` directory located at the root of the repository. You can store the files in the structure you prefer, but as an example, one could want them in the following structure under the data directory:\n",
    "\n",
    "- CPSC Database and CPSC-Extra Database\n",
    "- St. Petersberg (INCART) Database\n",
    "- PTB and PTB-XL Database\n",
    "- The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "- Chapman-Shaoxing and Ningbo Database\n",
    "\n",
    "To begin, let's retrieve the names of the `tar.gz` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b1e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WFDB_CPSC2018.tar.gz\n",
      "1 WFDB_CPSC2018_2.tar.gz\n",
      "2 WFDB_ChapmanShaoxing.tar.gz\n",
      "3 WFDB_Ga.tar.gz\n",
      "4 WFDB_Ningbo.tar.gz\n",
      "5 WFDB_PTB.tar.gz\n",
      "6 WFDB_PTBXL.tar.gz\n",
      "7 WFDB_StPetersburg.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# All tar.gz files (in the current working directory)\n",
    "curr_path = os.getcwd()\n",
    "targz_files = [file for file in os.listdir(curr_path) if os.path.isfile(os.path.join(curr_path, file)) and file.endswith('tar.gz') and file.startswith('WFDB')]\n",
    "\n",
    "# Let's sort the files\n",
    "targz_files = sorted(targz_files)\n",
    "\n",
    "for i, file in enumerate(targz_files):\n",
    "    print(i, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560e002",
   "metadata": {},
   "source": [
    "To follow the structure outlined above, the listed `tar.gz` files will be extracted as follows:\n",
    "\n",
    "* WFDB_CPSC2018.tar.gz + WFDB_CPSC2018_2.tar.gz\n",
    "* WFDB_StPetersburg.tar.gz\n",
    "* WFDB_PTB.tar.gz + WFDB_PTBXL.tar.gz\n",
    "* WFDB_Ga.tar.gz\n",
    "* WFDB_ChapmanShaoxing.tar.gz + WFDB_Ningbo.tar.gz\n",
    "\n",
    "Let's create a subdirectory named as `physionet_data` for the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131b041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz')\n",
      "('WFDB_StPetersburg.tar.gz',)\n",
      "('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz')\n",
      "('WFDB_Ga.tar.gz',)\n",
      "('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "# Let's make the split as tuples of tar.gz files\n",
    "# NB! If the split mentioned above wanted, SORTING is really important!\n",
    "tar_split = [(targz_files[0], targz_files[1]),\n",
    "             (targz_files[7], ),\n",
    "             (targz_files[5], targz_files[6]),\n",
    "             (targz_files[3], ),\n",
    "             (targz_files[2], targz_files[4])]\n",
    "\n",
    "print(*tar_split, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f886269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract files from a given tar to a given directory\n",
    "# Will exclude subdirectories from a given tar and load all the files directly to the given directory\n",
    "def extract_files(tar, directory):\n",
    "    \n",
    "    file = tarfile.open(tar, 'r')\n",
    "    \n",
    "    n_files = 0\n",
    "    for member in file.getmembers():\n",
    "        if member.isreg(): # Skip if the TarInfo is not file\n",
    "            member.name = os.path.basename(member.name) # Reset path\n",
    "            file.extract(member, directory)\n",
    "            n_files += 1\n",
    "    \n",
    "    file.close() \n",
    "    re_dir = re.search('data.*', directory)[0]\n",
    "    print('- {} files extracted to {}'.format(n_files, './'+re_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69e560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar.gz file(s) ('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz') to the CPSC_CPSC-Extra directory\n"
     ]
    },
    {
     "ename": "ReadError",
     "evalue": "file could not be opened successfully:\n- method gz: ReadError('empty file')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('empty file')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tar) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# More than one database in tuple\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m one_tar \u001b[38;5;129;01min\u001b[39;00m tar:\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mextract_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_tar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_tmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Only one database in tuple\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     extract_files(tar[\u001b[38;5;241m0\u001b[39m], save_tmp)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mextract_files\u001b[0;34m(tar, directory)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_files\u001b[39m(tar, directory):\n\u001b[0;32m----> 5\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     n_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m member \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetmembers():\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m#if member.isreg(): # Skip if the TarInfo is not file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m#member.name = os.path.basename(member.name) # Reset path\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m#file.extract(member, directory)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/physionet_model/lib/python3.10/tarfile.py:1629\u001b[0m, in \u001b[0;36mTarFile.open\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     error_msgs_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m-> 1629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile could not be opened successfully:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msgs_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1632\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mReadError\u001b[0m: file could not be opened successfully:\n- method gz: ReadError('empty file')\n- method bz2: ReadError('not a bzip2 file')\n- method xz: ReadError('not an lzma file')\n- method tar: ReadError('empty file')"
     ]
    }
   ],
   "source": [
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# Path to the physionet_data directory, i.e., save the dataset here\n",
    "data_path = os.path.join(abs_path.parent.absolute(), 'data', 'physionet_data')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Directories to which extract the data\n",
    "# NB! Gotta be at the same length than 'tar_split'\n",
    "dir_names = ['CPSC_CPSC-Extra', 'INCART', 'PTB_PTBXL', 'G12EC', 'ChapmanShaoxing_Ningbo']\n",
    "\n",
    "# Extracting right files to right subdirectories\n",
    "for tar, directory in zip(tar_split, dir_names):\n",
    "    \n",
    "    print('Extracting tar.gz file(s) {} to the {} directory'.format(tar, directory))\n",
    "    \n",
    "    # Saving path for the specific files\n",
    "    save_tmp = os.path.join(data_path, directory)\n",
    "    # Preparing the directory\n",
    "    if not os.path.exists(save_tmp):\n",
    "        os.makedirs(save_tmp)\n",
    "        \n",
    "    if len(tar) > 1: # More than one database in tuple\n",
    "        for one_tar in tar:\n",
    "            extract_files(one_tar, save_tmp)\n",
    "    else: # Only one database in tuple\n",
    "        extract_files(tar[0], save_tmp)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b35f9",
   "metadata": {},
   "source": [
    "Now total of **176 506** files (based on the data exploration presented earlier) should be located in the `physionet_data` directory. One ECG recording consists of a binary MATLAB v4 file and a text file in header format. To verify this count, you can easily perform a file count as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 176506 files\n"
     ]
    }
   ],
   "source": [
    "total_files = 0\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    total_files += len(files)\n",
    "    \n",
    "print('Total of {} files'.format(total_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec35b911",
   "metadata": {},
   "source": [
    "### <font color = teal> 1.2) Shandong Provincial Hospital and other sources </font> <a id='sph'></a>\n",
    "\n",
    "New data can be downloaded and utilized with this repository, provided that the following guidelines are followed:\n",
    "\n",
    "1\\) ECG data can be in either `MATLAB v4` (.mat) or `h5` formats. When setting up training or testing, ECGs are loaded into `torch.utils.data.Dataset` using the following fuction from the [`dataset_utils.py`](../src/dataloader/dataset_utils.py) script:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "def load_data(case):\n",
    "    ''' Load a MATLAB v4 file or a H5 file of an ECG recording\n",
    "    '''\n",
    "\n",
    "    if case.endswith('.mat'):\n",
    "        x = loadmat(case)\n",
    "        return np.asarray(x['val'], dtype=np.float64)\n",
    "    else:\n",
    "        with h5py.File(case) as f:\n",
    "            x = f['ecg'][()]\n",
    "        return np.asarray(x, dtype=np.float64)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "In both cases, there should be either a `val` column in the MATLAB file or an `ecg` column in the `h5` file.\n",
    "\n",
    "2\\) Demographic data, including diagnoses, age and gender, are loaded from either `WFDB header format` format (.hea) or from CSV files. These files also contain other essential information about the ECGs, such as sample frequency. They are required when generating CSV files using the [`create_data_csvs.py`](../create_data_csvs.py) script. Header files have a structure similar to the one shown below:\n",
    "\n",
    "<br>\n",
    "\n",
    "```text\n",
    "JS00001 12 500 5000 23-Mar-2021 20:20:47\n",
    "JS00001.mat 16+24 1000/mV 16 0 -254 21756 0 I\n",
    "JS00001.mat 16+24 1000/mV 16 0 264 -599 0 II\n",
    "JS00001.mat 16+24 1000/mV 16 0 517 -22376 0 III\n",
    "JS00001.mat 16+24 1000/mV 16 0 -5 28232 0 aVR\n",
    "JS00001.mat 16+24 1000/mV 16 0 -386 16619 0 aVL\n",
    "JS00001.mat 16+24 1000/mV 16 0 390 15121 0 aVF\n",
    "JS00001.mat 16+24 1000/mV 16 0 -98 1568 0 V1\n",
    "JS00001.mat 16+24 1000/mV 16 0 -312 -32761 0 V2\n",
    "JS00001.mat 16+24 1000/mV 16 0 -98 32715 0 V3\n",
    "JS00001.mat 16+24 1000/mV 16 0 810 15193 0 V4\n",
    "JS00001.mat 16+24 1000/mV 16 0 810 14081 0 V5\n",
    "JS00001.mat 16+24 1000/mV 16 0 527 32579 0 V6\n",
    "#Age: 85\n",
    "#Sex: Male\n",
    "#Dx: 164889003,59118001,164934002\n",
    "#Rx: Unknown\n",
    "#Hx: Unknown\n",
    "#Sx: Unknown\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The third value in the first row is the sample frequency, and age, gender and diagnoses are gotten from the lines 14-16. The 12 lines after the first one are corresponding the 12 leads of the ECG recordings, `Rx` is refering to the medical prespriction, `Hx` to the medical history and `Sx` to symptom or surgery. \n",
    "\n",
    "For the Shandong Provincial Hospital dataset, the demographic metadata is stored as a CSV file. The CSV file has a structure shown below:\n",
    "\n",
    "<br>\n",
    "\n",
    "ECG_ID | AHA_Code |Patient_ID|Age|Sex|N|Date\n",
    "-------|----------|----------|---|---|-|---\n",
    "A00001|22;23|S00001|55|M|5000|2020-03-04\n",
    "A00002|1|S00002|32|M|6000|2019-09-03\n",
    "A00003|1|S00003|63|M|6500|2020-07-16\n",
    "A00004|23|S00004|31|M|5000|2020-07-14\n",
    "...|...|...|...|...|...|...|\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that whether the metadata is in a CSV file or in a header file, <font color = red><i>all metadata files should be located in the same directory than the corresponding ECG files</i></font>. Also note, that <font color = red>the CSV files and header files should <b>always</b> contain the similarly named columns like `Age`, `Sex` and `Gender`</font>.\n",
    "\n",
    "3\\) It is highly recommended to download data directly into the `data` directory. Several predefined paths within the repository are configured to point to this directory, particularly when creating CSV files or YAML files for configuring training and testing.\n",
    "\n",
    "Additionally, the code provided above can be used for extracting tar.gz files. The `extract_files(tar, save_path)` function is a general-purpose tool for this task, where `tar` represents the tar.gz file to be extracted, and `save_path` indicates the absolute path to which the file is extracted. For example, the following code snippet demonstrates its usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edd6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other sources\n",
    "## -------------\n",
    "'''\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# The name of the tar gz file (located in the current directory)\n",
    "tar = 'records.tar.gz'\n",
    "save_path = os.path.join(abs_path.parent.absolute(), 'data', 'Shandong')\n",
    "#extract_files(tar, save_path)\n",
    "\n",
    "# If needed, the samples can be renamed\n",
    "samples = sorted(os.listdir(save_path))  # get the current names of the samples\n",
    "path_samples = sorted([os.path.join(save_path, s) for s in samples]) # add path to the current names\n",
    "new_names = [name.replace('A', 'SPH') for name in samples] # rename the beginning of the file (e.g., A0001.h5 to SPH0001.h5)\n",
    "path_new_names = sorted([os.path.join(save_path, nn) for nn in new_names]) # add path the the old names\n",
    "\n",
    "# Rename samples\n",
    "for old, new in zip(path_samples, path_new_names):\n",
    "    os.rename(old, new)\n",
    "\n",
    "# Also, if csv file of metadata has the IDs too, change them if needed\n",
    "csv_file = pd.read_csv('metadata.csv')\n",
    "csv_file ['ECG_ID'] = [s.replace('.h5', '') for s in new_names]\n",
    "\n",
    "# Be also sure that we have a sample frequency in it\n",
    "csv_file['fs'] = 500\n",
    "csv_file.to_csv(os.path.join(save_path, 'metadata.csv'), index=False)\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d92f3c0",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## <font color = teal>2) Label mapping </font> <a id='map'></a>\n",
    "\n",
    "The primary diagnostic code system utilized in this repository is SNOMED CT Codes.\n",
    "\n",
    "Given that ECGs can be labeled with various code systems, we provide the [`label_mapping.py`](../label_mapping.py) script to facilitate the conversion of non-SNOMED CT Codes. It's important to note that the script assumes the metadata for a specific dataset is available in a CSV file. The core concept of this script is to map the labels extracted from the metadata file using the [`AHA_SNOMED_mapping.csv`](../data/AHA_SNOMED_mapping.csv) file (located in the `data` directory) and then add the corresponding SNOMED CT Code to an additional column named `SNOMEDCTCodes. The remaining content of the metadata file remains unaltered.\n",
    "\n",
    "The `AHA_SNOMED_mapping.csv` file contains diagnostic statements conforming to the AHA standard along with their corresponding SNOMED CT Codes in the following format:\n",
    "\n",
    "<br>\n",
    "\n",
    "Dx|SNOMEDCTCode|AHA_Code\n",
    "------|---------|-------------\n",
    "1st degree av block|270492004|82\n",
    "prolonged pr interval|164947007|82\n",
    "atrial fibrillation|164889003|50\n",
    "atrial fibrillation|164889003|50+346\n",
    "atrial fibrillation|164889003|50+347\n",
    "atrial flutter|164890007|51\n",
    "incomplete right bundle branch block|713426002|105\n",
    "... | ... | ...\n",
    "\n",
    "<br>\n",
    "\n",
    "New codes can be added to this file without issue, as long as the structure of the CSV file is maintained. The `Dx` column, while not used for mapping, serves as a helpful reference for diagnoses. A new CSV file for the updated metadata will be saved at the same location from where the original metadata file has been loaded. This needs to considered, as <i>there can be only one CSV file per data folder</i>. Thus, either remove or move the metadata CSV file which is not used in the later phases.\n",
    "\n",
    "By default, the [`label_mapping.py`](../label_mapping.py) script is set to convert AHA statements into SNOMED CT Codes. There are attributes that need to be considered before running the script:\n",
    "\n",
    "1) There are two paths for CSV files: `csv_path` refers to the path from where the file metadata file for the Shandong Provincial Hospital dataset is found. `csv_save_path` refers to the location and the filename to which the updated metadata file will be stored. By default, it's set to point to the metadata file of the Shandong Provincial hospital data in the `smoke_data` directory, i.e., `./data/smoke_data/SPH/metadata.csv`. \n",
    "\n",
    "2) The `map_path` attribute refers to the location from where the file containing the mapping between AHA and SNOMED standards is found. By default, it is set to use the provided mapping csv file in the `data` directory, i.e., `./data/AHA_SNOMED_mapping.csv`.\n",
    "\n",
    "3) The `from_code` attribute refers to the diagnostic standard which is initially converted into some other standard. By default, it is set to AHA statements, i.e., these will be converted to SNOMED CT Codes.\n",
    "\n",
    "4) The `to_code` attribute refers to the diagnostic standard into which some other diagnostic standard is likely to be converted. By default it is set to SNOMED CT codes, i.e., this is the standard into which the AHA statements are converted. \n",
    "\n",
    "5) The `imputation` attribute defines whether a sinus rhythm label is needed to be imputed to the Shandong Provincial Hospital data. The imputation is recommended if the sinus rhythm label will be used in classification. The basic idea behind the imputation is to fit a Logistic Regression model with PhysioNet 2021 data (which contains the sinus rhythm label) and this model then is used to predict the sinus rhythm label for the Shandong Provincial Hospital dataset. By default, this is set to `True`, i.e., the sinus rhythm labels are imputed to the Shandong Provincial Hospital dataset.\n",
    "\n",
    "6) The `input_dir` attribute refers to the path from where the data is loaded to fit the Logistic Regression model. By default, this is set to the location of the PhysioNet 2021 data, i.e., it is used to fit the Logistic Regression model.\n",
    "\n",
    "### <font color = teal> Terminal command </font>\n",
    "\n",
    "To use the script, simply run the following command:\n",
    "\n",
    "```\n",
    "python label_mapping.py\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f2cd392",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## <font color = teal> 3) Preprocessing data (optional) </font> <a id='data_prep'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23022168",
   "metadata": {},
   "source": [
    "You can preprocess all the data using various transformations with the [`preprocess_data.py`](../preprocess_data.py) script. There are two critical attributes to consider:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# Original data location\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'smoke_data')\n",
    "\n",
    "# New location for preprocessed data\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_smoke_data')\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "The `from_directory` points to the directory where the data in its original format is loaded from, such as the downloaded Physionet Challenge 2021 data. The `new_directory` indicates the new location where the preprocessed ECGs will be saved. It's worth noting that <i>when saving the preprocessed ECGs, the metadata files (e.g., a CSV file or each corresponding .hea file) must also be copied to that location</i>. The script takes care of this task as well.\n",
    "\n",
    "By default, two transforms are applied:\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# ------------------------------\n",
    "# --- PREPROCESS TRANSFORMS ----\n",
    "new_fs = 250\n",
    "\n",
    "# - BandPass filter \n",
    "bpf = BandPassFilter(fs = ecg_fs)\n",
    "ecg = bpf(ecg)\n",
    "\n",
    "# - Linear interpolation\n",
    "linear_interp = Linear_interpolation(fs_new = new_fs, fs_old = ecg_fs)\n",
    "ecg = linear_interp(ecg)\n",
    "\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "It's important to note that the preprocessing step **is not mandatory for the repository to function**. However, if you plan to use transforms, like the two mentioned above, during the training phase, it's advisable to preprocess the data beforehand using the provided script. This can significantly improve training efficiency.\n",
    "\n",
    "Other transformations are defined in the [`dataset.py`](../src/dataloader/dataset.py) script located in `src/dataloader/`, which is executed during training. Additionally, several transformations can be found in the [`transforms.py`](../src/dataloader/transforms.py) script, including mentioned `Linear_interpolation` and `BandPassFilte`, which are in the same directory.\n",
    "\n",
    "### <font color = teal> Terminal command </font>\n",
    "\n",
    "To use the script, simply run the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!** The preprocessed ECGs will have different names compared to the original ones, so it's important to keep track of whether the preprocessing step has been completed or not</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e0d492",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 4) Splitting data into CSV files </font> <a id='split'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f439d39e",
   "metadata": {},
   "source": [
    "The entire data splitting process is managed by the [`create_data_csvs.py`](../create_data_csvs.py) script. The primary objective of this script is to divide the data into CSV files, which can later be utilized for both training and testing purposes.\n",
    "\n",
    "The CSV files possess the following columns: `path` (the path to a specific ECG recording), `age`, `gender`, and all the diagnoses represented in SNOMED CT codes, which are employed as labels for classification. A value of 1 indicates that the patient has a particular disease. The structure of these CSV files is as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "| path  | age  | gender  | 10370003  | 111975006 | 164890007 | *other diagnoses...* |\n",
    "| ------------- |-------------|-------------| ------------- |-------------|-------------|-------------|\n",
    "| ./data/A0002.mat | 49.0 | Female | 0 | 0 | 1 | ... |\n",
    "| ./data/A0003.mat | 81.0 | Female | 0 | 1 | 1 | ... |\n",
    "| ./data/A0004.mat | 45.0 |  Male  | 1 | 0 | 0 | ... |\n",
    "| ... | ... |  ...  | ... | ... | ... | ... |\n",
    "\n",
    "<br>\n",
    "\n",
    "Before running the script, several attributes must be carefully configured within the main block:\n",
    "\n",
    "1) The `stratified` attribute is used to specify the type of data split. If set to `True`, the script performs a stratified data split; if set to `False`, a database-wise split is executed.\n",
    "\n",
    "2) The `data_dir` attribute should be set to point to the correct data directory from which the data is loaded. By default, it's configured to load data from the `smoke_data` directory, which is a subdirectory of the data directory.\n",
    "\n",
    "3) The `csv_dir` attribute should be set to specify the desired folder where the generated CSV files will be saved. This directory will be created under the `split_csv` directory which is a subdirectory of the `data` directory. By default, CSV files are saved in the `smoke_stratified_shuffle` folder.\n",
    "\n",
    "4) The class labels are needed to be set with the `labels` attribute in the script. By default, the labels are set as a list containing `426783006`, `426177001`, `427084000`, `164890007`, `164889003`, `427393009`, `164947007` and `270492004`. \n",
    "\n",
    "5) The `cv_type` attributes defines which kind of cross validation is used: ShuffleSplit or K-Fold. By default, it's set to `shufflesplit`.\n",
    "\n",
    "6) If K-Fold cross validation is selected, the `cv_k` attributes defines the k value. By default, the k value is set to `5`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c76a439",
   "metadata": {},
   "source": [
    "The data can be split in two different ways:\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. Above, the data was extracted into the following structure:\n",
    "\n",
    "   * CPSC Database and CPSC-Extra Database\n",
    "   * St. Petersberg (INCART) Database\n",
    "   * PTB and PTB-XL Database\n",
    "   * The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "   * Chapman-Shaoxing and Ningbo Database\n",
    "   \n",
    " The `dbwise_csvs(data_directory, save_directory, labels)` function leverages this structure to create CSV files. The `data_directory` parameter indicates the location of the data (note that subdirectories are considered as different databases), `save_directory` specifies where the CSV files will be saved, and `labels` lists the SNOMED CT coded labels used for classification. CSV files are named according to the directories from which they were created, e.g., a CSV file for CPSC Database and CPSC-Extra Database is named `CPSC_CPSC-Extra.csv`.\n",
    "\n",
    " Since models read only one CSV file to obtain the paths of the ECGs during training and testing, there may be a need to combine multiple databases into a single CSV file. For instance, if CPSC-Extra, CPSC, G12EC, PTB, and PTB XL are used for training, these combinations of different databases can be created in the script. However, there's an assumption behind the split: as the `data_directory` parameter is given, from where the names of the databases (subdirectories) are read, <i>one is considered as a test set, one as a validation set, and all the others as a training set</i>.\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>. The `stratified_csvs(data_directory, save_directory, labels, train_test_splits)` function performs a stratified split. The parameters are similar to those in the `dbwise_csvs` function, but there's an additional parameter, `train_test_splits`, which is a dictionary specifying the train-test splits. The dictionary is structured as a collection of dictionaries, where the internal directories refer to specific train-test splits. For example, there is one train-test split set in the train_test_splits dictionary by default, as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "   ```python\n",
    "   train_test_splits = {\n",
    "   'split_1': {    \n",
    "         'train': ['G12EC', 'SPH', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "         'test': ['CPSC_CPSC-Extra']\n",
    "      }\n",
    "   }\n",
    "   ```\n",
    "<br>\n",
    "\n",
    "In this example, `split_1` is simply a name for this particular split, and it contains keys for train and test to specify which databases are considered as training data and which ones as test data. Training data is further divided into training and validation sets. Names (e.g., split_1) are used to identify the CSV files. Note that <i>`train` and `test` keys should have values set as a list, even though there would be only one database defined</i>!\n",
    "\n",
    "Stratification itself is performed using one of the two the multilabel cross-validators from the `iterative-stratification` package: `MultilabelStratifiedShuffleSplit(n_splits, test_size, train_size, random_state)` or `MultilabelStratifiedKFold(n_splits, shuffle, random_state)`. In the case of ShuffleSplit, the script will use the number of splits (`n_splits`) equal to the length of the training dataset (in our case, it will be 4 as data is gathered from 'G12EC', 'SPH', 'PTB_PTBXL', and 'ChapmanShaoxing_Ningbo'). For K-Fold cross validation, the k value needs to be set manually. Additional information about this and other multilabel cross-validators can be found in [the GitHub repository of iterative-stratification](https://github.com/trent-b/iterative-stratification)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db16fe9b",
   "metadata": {},
   "source": [
    "### <font color = teal> About the naming of csv files </font>\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. The CSV files created through the database-wise split have intuitive names. They are named after the source database, for example, PTB_PTBXL.csv. The combined CSV files are named based on the combination of databases used to structure the CSV file. For instance, if the training data comes from CPSC/CPSC-Extra, SPH, and PTB/PTB-XL databases, the combined CSV files will be named `CPSC_CPSC-Extra_INCART_PTB_PTBXL.csv`.\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>.  Since there are four different data sources, four distinct data splits need to be created, each using one dataset as a testing set while the others serve as training sets. The [`create_data_csvs.py`](../create_data_csvs.py) script names the resulting CSV files based on information from the keys of the `train_test_splits` dictionary and the outcomes of the `MultilabelStratifiedShuffleSplit()` cross validator. For example, the CSV file names could be as follows:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "train_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp val_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp test_split_1.csv\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "First, the CSV files are categorized into `train`, `val`, and `test` subsets. Then, as the `train_test_splits` dictionary employs keys for indexing the splits (e.g., `split_1` and `split_2`), the first indeces correspond to this indexing. The latter indeces refer to the results of the `MultilabelStratifiedShuffleSplit()` cross-validator: Since data is collected from four different databases and stratified, it generates four different splits for the training and validation sets. Therefore, the latter indexing reflects the functionality of the mentioned cross-validator. The same applies to the naming of the CSV files by the `MultilabelStratifiedKFold` cross-validator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0399870e",
   "metadata": {},
   "source": [
    "### <font color = teal>  Terminal command </font>\n",
    "\n",
    "Once the necessary attributes are initialized, you can execute the desired data split using the following terminal command:\n",
    "\n",
    "```\n",
    "python create_data_csvs.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4fb7f1",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## <font color = teal> Creation of the configuration files </font> <a id='yamls'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd62583",
   "metadata": {},
   "source": [
    "Once you have mapped and preprocessed the data and created the CSV files to tell the models the paths of the ECGs, the labels and demographic features, the YAML file can be created using either step-by-step implementation in the notebooks [Yaml files of database-wise split for training and testing](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of stratified split for training and testing](2_physionet_stratified_yaml_files.ipynb) or the [`create_yaml_files.py`](../create_yaml_files.py) script.\n",
    "\n",
    "Before running the script, the following attributes need to be set:\n",
    "\n",
    "1) `csv_path`: From where the data split CV files are read. This will be stored into the YAML files so make sure that the folder exists.\n",
    "\n",
    "2) `train_yaml_save_path` and `test_yaml_save_path`: Where to store the YAML files for training and testing, respectively. The last argument in the path should only be changed so that the files are stored in `configs/training/` or `configs/predicting/`. \n",
    "\n",
    "3) `name`: The beginning for the YAML files. So if set as `split`, the YAML files will be names e.g. `split_1_1.yaml`.\n",
    "\n",
    "4) `train_dict` and `test_dict`: To set the parameters for the YAML files. Do <b>NOT</b> change the keys as there is listed all the necessary keys used in training and evaluation scipts. New keys can be added.\n",
    "\n",
    "The YAML files are created based on either the CSV files listed in the path set to `csv_path` or database-wise, when the databases are listed in the `data` attribute. There should be a pair of training and test files per each model. I.e., if you have splitted data into CSV files for training, validation and testing as follows\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "train_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp val_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp test_split_1.csv\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "you will create two yaml files, the first one below being for training and the second one for testing:\n",
    "\n",
    "```text\n",
    "train_file: train_split_1_1.csv\n",
    "val_file: val_split_1_1.csv\n",
    "csv_path: stratified_smoke\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "epochs: 1\n",
    "lr: 0.003000\n",
    "weight_decay: 0.000010\n",
    "device_count: 1\n",
    "threshold: 0.500000\n",
    "bandwidth:\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```text\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "csv_path: stratified_smoke\n",
    "device_count: 1\n",
    "threshold: 0.500000\n",
    "bandwidth:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be66f0e",
   "metadata": {},
   "source": [
    "### <font color = teal>  Terminal command </font>\n",
    "\n",
    "Once the necessary attributes are initialized, you can execute the yaml script using the following terminal command:\n",
    "\n",
    "```\n",
    "python create_yaml_files.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440b5d2",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## <font color = teal> Example: Creating CSV files from the provided smoke data </font> <a id='example'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3484469",
   "metadata": {},
   "source": [
    "*All the data files for smoke testing are available in the repository.*\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_dir` **attribute is set with the assumption that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as the** `smoke_data` **directory.** The paths for ECGs will be different in the csv files depending on whether preprocessing has been used or not.</font>\n",
    "\n",
    "First, we want to **preprocess the data**. Ensure that the [`preprocess_data.py`](../preprocess_data.py) script has the original and new directories set as follows\n",
    "\n",
    "```python\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'smoke_data')\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "The `from_directory` attribute refers to the directory where the original data is located, and the `new_directory` attribute refers to where the preprocessed data will be saved. You can perform preprocessing with the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "Once the data is preprocessed, you can proceed to **split the data into CSV files** using the [`create_data_csvs.py`](../create_data_csvs.py) script. Make sure the attributes are set as below before running the following command:\n",
    "\n",
    "```\n",
    "python create_data_csvs.py\n",
    "```\n",
    "\n",
    "###  <font color = forestgreen> Database-wise split </font>\n",
    "\n",
    "Ensure that the following attributes are set **before the if-else statement** as follows:\n",
    "\n",
    "```python\n",
    "stratified = False\n",
    "data_dir =  'preprocessed_smoke_data'\n",
    "csv_dir =  'dbwise_smoke'\n",
    "labels = ['426783006', '426177001', '427084000', '164890007', '164889003', '427393009', '164947007', '270492004']\n",
    "```\n",
    "\n",
    "The resulting CSV files will be saved in `./data/split_csvs/dbwise_smoke/` with the following files:\n",
    "\n",
    "```text\n",
    "ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_G12EC.csv\n",
    "ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_PTB_PTBXL.csv\n",
    "ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_SPH.csv\n",
    "ChapmanShaoxing_Ningbo_G12EC_PTB_PTBXL.csv\n",
    "ChapmanShaoxing_Ningbo_G12EC_SPH.csv\n",
    "ChapmanShaoxing_Ningbo_PTB_PTBXL_SPH.csv\n",
    "ChapmanShaoxing_Ningbo.csv\n",
    "...\n",
    "```\n",
    "\n",
    "To create the YAML files, run the [`create_yaml_files.py`](../create_yaml_files.py) script. For the dbwise splitted data, make sure that you set\n",
    "\n",
    "```python\n",
    "kfold = False\n",
    "csv_path = os.path.join(os.getcwd(), 'data', 'split_csvs', 'dbwise_smoke')\n",
    "```\n",
    "\n",
    "Then, you can store the YAML files in the folders `train_smoke_yamls` and `test_smoke_yamls` using `split` as the beginning of the filenames by setting\n",
    "\n",
    "```python\n",
    "train_yaml_save_path = os.path.join(os.getcwd(), 'configs', 'training', 'train_yamls_smoke')\n",
    "test_yaml_save_path = os.path.join(os.getcwd(), 'configs', 'predicting', 'test_yamls_smoke')\n",
    "name = 'split'\n",
    "```\n",
    "\n",
    "The YAML files for training and evaluation differ so they can be set e.g. as follows:\n",
    "\n",
    "```python\n",
    "train_dict = {\n",
    "        'csv_path': os.path.basename(csv_path),\n",
    "\n",
    "        # Training parameters\n",
    "        'batch_size': 10,\n",
    "        'num_workers': 0,\n",
    "        'epochs': 1,\n",
    "        'lr': 0.003,\n",
    "        'weight_decay': 0.00001,\n",
    "\n",
    "        # Device configurations\n",
    "        'device_count': 1,\n",
    "\n",
    "        # Decision threshold for predictions\n",
    "        'threshold': 0.5,\n",
    "\n",
    "        # For ECGs\n",
    "        'bandwidth': ''\n",
    "}\n",
    "    \n",
    "test_dict = {\n",
    "        \n",
    "        # Directory where the csv file for data split are in 'data/split_cvs/'\n",
    "        # (the same value as already set in `csv_path`, however, only the basename)\n",
    "        'csv_path': os.path.basename(csv_path),\n",
    "\n",
    "        # Device configurations\n",
    "        'device_count': 1,\n",
    "\n",
    "        # Decision threshold for predictions\n",
    "        'threshold': 0.5,\n",
    "\n",
    "        # For ECGs\n",
    "        'bandwidth': ''\n",
    "}   \n",
    "```\n",
    "\n",
    "Finally, the command to run the YAML script was the following:\n",
    "\n",
    "```\n",
    "python create_yaml_files.py\n",
    "```\n",
    "\n",
    "After execution, you should be able to find the YAML files for training and evaluation in `configs/training/` and `configs/predicting/`, respectively. There should be similarly named folders as the attributes `train_yaml_save_path` and `test_yaml_save_path` are set.\n",
    "\n",
    "### <font color = forestgreen> Stratified split </font>\n",
    "\n",
    "For stratified data split, you can use a dictionary of dictionaries to specify the desired train-test splits. By default, there is one split provided in the script:\n",
    "\n",
    "- Train data is from the directories G12EC, SPH, PTB_PTBXL, and ChapmanShaoxing_Ningbo.\n",
    "- Test data is from the directory CPSC_CPSC-Extra.\n",
    "\n",
    "Set the following attributes **before the if-else statement** as follows:\n",
    "\n",
    "```python\n",
    "stratified = True\n",
    "data_dir =  'preprocessed_smoke_data'\n",
    "csv_dir =  'stratified_smoke'\n",
    "labels = ['426783006', '426177001', '427084000', '164890007', '164889003', '427393009', '164947007', '270492004']\n",
    "```\n",
    "\n",
    "Specify the databases used for training and testing by setting the `train_test_splits` attribute **within the if block**:\n",
    "\n",
    "```python\n",
    "train_test_splits = {\n",
    "    'split_1': {    \n",
    "        'train': ['G12EC', 'SPH', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "        'test': ['CPSC_CPSC-Extra']\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The resulting csv files will be saved in `./data/split_csvs/stratified_smoke/` with names like:\n",
    "\n",
    "```text\n",
    "test_split1.csv\n",
    "train_split_1_1.csv\n",
    "train_split_1_2.csv\n",
    "train_split_1_3.csv\n",
    "train_split_1_4.csv\n",
    "val_split_1_1.csv\n",
    "val_split_1_2.csv\n",
    "val_split_1_3.csv\n",
    "val_split_1_4.csv\n",
    "```\n",
    "\n",
    "As with the dbwise-splitted data, you can create the YAML files by running the [`create_yaml_files.py`](../create_yaml_files.py) script. You can set all the other attributes as shown with the dbwise-splitted data, but change the followings:\n",
    "\n",
    "```python\n",
    "kfold = True\n",
    "csv_path = os.path.join(os.getcwd(), 'data', 'split_csvs', 'stratified_smoke')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "af433470baea3cbfb1d2a9219a544bb72a17c8a5091280fdb93be39946c5da4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
