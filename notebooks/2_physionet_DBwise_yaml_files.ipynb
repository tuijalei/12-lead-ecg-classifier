{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7999d565",
   "metadata": {},
   "source": [
    "# <font color = teal> Yaml files of database-wise split for training and testing </font>\n",
    "\n",
    "With this notebook, you can create the yaml files needed in training and testing with a data split which is made **database-wise**. The so-called \"original data split\" should be first made with the script `create_data_csvs.py`. This ensures that there are already one csv file for each database. The combined csv files are created below. More detailed information about this is available in the notebook [Introduction to data handling](1_introduction_data_handling.ipynb). \n",
    "\n",
    "------\n",
    "\n",
    "Note that the hyperparameters considering training and testing are set in the yaml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d351bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters for the yaml files -------------\n",
    "# Training parameters\n",
    "batch_size = 10\n",
    "num_workers = 0\n",
    "epochs = 1\n",
    "lr = 0.003\n",
    "weight_decay = 0.00001\n",
    "\n",
    "# Device configurations\n",
    "device_count = 1\n",
    "\n",
    "# Decision threshold for predictions\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2322c",
   "metadata": {},
   "source": [
    "Examples of the training and the testing yaml files are provided below.\n",
    "\n",
    "**Yaml file for training a model**\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "train_file: train_split_1_1.csv\n",
    "val_file: val_split_1_1.csv\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "batch_size: 10\n",
    "num_workers: 0\n",
    "epochs: 1\n",
    "lr: 0.003000\n",
    "weight_decay: 0.000010\n",
    "\n",
    "# VALIDATION SETTINGS\n",
    "threshold = 0.5\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```\n",
    "\n",
    "**Yaml file for testing a model**\n",
    "\n",
    "```\n",
    "# INITIAL SETTINGS\n",
    "test_file: test_split_1.csv\n",
    "model: split_1_1.pth\n",
    "\n",
    "# TESTING SETTINGS\n",
    "threshold: 0.500000\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "device_count: 1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2f21607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from ruamel.yaml import YAML\n",
    "from itertools import combinations\n",
    "\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# PARAMETERS TO CREATE DBWISE YAML FILES  \n",
    "# ----------------------------------------\n",
    "\n",
    "# From where to load the csv files of \"original data split\"\n",
    "# Note that this is the saving location for combined csv files too\n",
    "csv_path = os.path.join(abs_path.parent.absolute(), 'data', 'split_csvs', 'dbwise_smoke')\n",
    "\n",
    "# Where to save the training yaml files\n",
    "train_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'training', 'train_dbwise_smoke')\n",
    "\n",
    "# Where to save the testing yaml files\n",
    "test_yaml_save_path = os.path.join(abs_path.parent.absolute(), 'configs', 'predicting', 'predict_dbwise_smoke')\n",
    "\n",
    "# The files that are used for training, validation and test sets\n",
    "data = ['PTB_PTBXL.csv', 'Shandong.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv']\n",
    "\n",
    "# Name for yaml files given as a string\n",
    "# names will be formed as <name><index>.yaml\n",
    "name = 'split_'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a20b7fe9",
   "metadata": {},
   "source": [
    "## <font color = teal> Combinations of different databases for training, validation and test sets </font>\n",
    "\n",
    "Yaml files are created based on the csv files listed above. Yaml files can be divided into training, validation and testing yaml files. All the possible splits of Physionet 2021 data are as follows\n",
    "\n",
    "**CPSC_CPSC-Extra.csv as test data**\n",
    "\n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "    2) train: PTB_PTBXL.csv, INCART.csv, ChapmanShaoxing_Ningbo.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: PTB_PTBXL.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, ChapmanShaoxing_Ningbo.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "**ChapmanShaoxing_Ningbo.csv as test data**\n",
    "\n",
    "    1) train: PTB_PTBXL.csv, INCART.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: PTB_PTBXL.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv, \n",
    "\n",
    "**PTB_PTBXL.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, INCART.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, INCART.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    4) train: INCART.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "**INCART.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, G12EC.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "       val: G12EC.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "    4) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, G12EC.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv\n",
    "\n",
    "**G12EC.csv as test data**\n",
    "\n",
    "    1) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, INCART.csv\n",
    "       val: CPSC_CPSC-Extra.csv\n",
    "\n",
    "    2) train: ChapmanShaoxing_Ningbo.csv, PTB_PTBXL.csv, CPSC_CPSC-Extra.csv\n",
    "       val: INCART.csv\n",
    "\n",
    "    3) train: ChapmanShaoxing_Ningbo.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "       val: PTB_PTBXL.csv\n",
    "\n",
    "    4) train: PTB_PTBXL.csv, CPSC_CPSC-Extra.csv, INCART.csv\n",
    "       val: ChapmanShaoxing_Ningbo.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2775f96",
   "metadata": {},
   "source": [
    "Let's make a function to find the combinations which are set in the `training_data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4c3f9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of all the train-val-test combinations: 20\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'G12EC.csv'], 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv']\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'ChapmanShaoxing_Ningbo.csv'], 'G12EC.csv', 'CPSC_CPSC-Extra.csv']\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], 'Shandong.csv', 'CPSC_CPSC-Extra.csv']\n",
      "[['Shandong.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], 'PTB_PTBXL.csv', 'CPSC_CPSC-Extra.csv']\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'G12EC.csv'], 'CPSC_CPSC-Extra.csv', 'ChapmanShaoxing_Ningbo.csv']\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'CPSC_CPSC-Extra.csv'], 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv']\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'], 'Shandong.csv', 'ChapmanShaoxing_Ningbo.csv']\n",
      "[['Shandong.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'], 'PTB_PTBXL.csv', 'ChapmanShaoxing_Ningbo.csv']\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'ChapmanShaoxing_Ningbo.csv'], 'CPSC_CPSC-Extra.csv', 'G12EC.csv']\n",
      "[['PTB_PTBXL.csv', 'Shandong.csv', 'CPSC_CPSC-Extra.csv'], 'ChapmanShaoxing_Ningbo.csv', 'G12EC.csv']\n",
      "[['PTB_PTBXL.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'Shandong.csv', 'G12EC.csv']\n",
      "[['Shandong.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'PTB_PTBXL.csv', 'G12EC.csv']\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], 'CPSC_CPSC-Extra.csv', 'Shandong.csv']\n",
      "[['PTB_PTBXL.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'], 'ChapmanShaoxing_Ningbo.csv', 'Shandong.csv']\n",
      "[['PTB_PTBXL.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'G12EC.csv', 'Shandong.csv']\n",
      "[['G12EC.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'PTB_PTBXL.csv', 'Shandong.csv']\n",
      "[['Shandong.csv', 'G12EC.csv', 'ChapmanShaoxing_Ningbo.csv'], 'CPSC_CPSC-Extra.csv', 'PTB_PTBXL.csv']\n",
      "[['Shandong.csv', 'G12EC.csv', 'CPSC_CPSC-Extra.csv'], 'ChapmanShaoxing_Ningbo.csv', 'PTB_PTBXL.csv']\n",
      "[['Shandong.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'G12EC.csv', 'PTB_PTBXL.csv']\n",
      "[['G12EC.csv', 'ChapmanShaoxing_Ningbo.csv', 'CPSC_CPSC-Extra.csv'], 'Shandong.csv', 'PTB_PTBXL.csv']\n"
     ]
    }
   ],
   "source": [
    "tvt_combs = []\n",
    "\n",
    "# Find all combinations of the spesified data (= csv files of the databases)\n",
    "# One is left for testing so taking combinations in size of len(data) -1\n",
    "for train_val_set in combinations(data, len(data)-1):\n",
    "    test = next(file for file in data if not file in train_val_set)\n",
    "\n",
    "    # And one is left for validation set so len(combinations took within first loop) -1\n",
    "    for train_set in combinations(train_val_set, len(train_val_set)-1):\n",
    "        val = next(file for file in data if not file in train_set and file != test)\n",
    "        tvt_combs.append([list(train_set), val, test])\n",
    "\n",
    "print('The number of all the train-val-test combinations:', len(tvt_combs))\n",
    "print(*tvt_combs, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3539b31",
   "metadata": {},
   "source": [
    "All the different training, validation and testing splits are stored in the `combinations_data` attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e08de4",
   "metadata": {},
   "source": [
    "## <font color = teal> Yaml files </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5545131",
   "metadata": {},
   "source": [
    "The csv files for different combinations are already made with the `create_data_csvs.py` script.  All the yaml files for training will be saved to `/configs/training`. The yaml files for testing will be saved to `/configs/predicting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a317ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 names created, for example ['split_1.yaml', 'split_2.yaml', 'split_3.yaml']\n"
     ]
    }
   ],
   "source": [
    "# Create as many names as there are train-val-test splits\n",
    "split_names = []\n",
    "for i in range(len(tvt_combs)):\n",
    "    split_names.append(name + str(i+1) + '.yaml')  \n",
    "    \n",
    "print(f'{len(split_names)} names created, for example {split_names[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa36ae7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: `G12EC_PTB_PTBXL_Shandong.csv`, validation data: `ChapmanShaoxing_Ningbo.csv`, test data: `CPSC_CPSC-Extra.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_PTB_PTBXL_Shandong.csv`, validation data: `G12EC.csv`, test data: `CPSC_CPSC-Extra.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_G12EC_PTB_PTBXL.csv`, validation data: `Shandong.csv`, test data: `CPSC_CPSC-Extra.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_G12EC_Shandong.csv`, validation data: `PTB_PTBXL.csv`, test data: `CPSC_CPSC-Extra.csv`\n",
      "\n",
      "Training data: `G12EC_PTB_PTBXL_Shandong.csv`, validation data: `CPSC_CPSC-Extra.csv`, test data: `ChapmanShaoxing_Ningbo.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_PTB_PTBXL_Shandong.csv`, validation data: `G12EC.csv`, test data: `ChapmanShaoxing_Ningbo.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_G12EC_PTB_PTBXL.csv`, validation data: `Shandong.csv`, test data: `ChapmanShaoxing_Ningbo.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_G12EC_Shandong.csv`, validation data: `PTB_PTBXL.csv`, test data: `ChapmanShaoxing_Ningbo.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_PTB_PTBXL_Shandong.csv`, validation data: `CPSC_CPSC-Extra.csv`, test data: `G12EC.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_PTB_PTBXL_Shandong.csv`, validation data: `ChapmanShaoxing_Ningbo.csv`, test data: `G12EC.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_PTB_PTBXL.csv`, validation data: `Shandong.csv`, test data: `G12EC.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_Shandong.csv`, validation data: `PTB_PTBXL.csv`, test data: `G12EC.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_G12EC_PTB_PTBXL.csv`, validation data: `CPSC_CPSC-Extra.csv`, test data: `Shandong.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_G12EC_PTB_PTBXL.csv`, validation data: `ChapmanShaoxing_Ningbo.csv`, test data: `Shandong.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_PTB_PTBXL.csv`, validation data: `G12EC.csv`, test data: `Shandong.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_G12EC.csv`, validation data: `PTB_PTBXL.csv`, test data: `Shandong.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_G12EC_Shandong.csv`, validation data: `CPSC_CPSC-Extra.csv`, test data: `PTB_PTBXL.csv`\n",
      "\n",
      "Training data: `CPSC_CPSC-Extra_G12EC_Shandong.csv`, validation data: `ChapmanShaoxing_Ningbo.csv`, test data: `PTB_PTBXL.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_Shandong.csv`, validation data: `G12EC.csv`, test data: `PTB_PTBXL.csv`\n",
      "\n",
      "Training data: `ChapmanShaoxing_Ningbo_CPSC_CPSC-Extra_G12EC.csv`, validation data: `Shandong.csv`, test data: `PTB_PTBXL.csv`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def save_yaml(yaml_str, yaml_path, i):\n",
    "    ''' Save the given string as a yaml file in the given location.\n",
    "    '''\n",
    "    # Make the yaml directory\n",
    "    if not os.path.isdir(yaml_path):\n",
    "        os.mkdir(yaml_path)\n",
    "    \n",
    "    # Write the yaml file\n",
    "    with open(os.path.join(yaml_path, split_names[i] ), 'w') as yaml_file:\n",
    "        yaml = YAML()\n",
    "        code = yaml.load(yaml_str)\n",
    "        yaml.dump(code, yaml_file)\n",
    "    \n",
    "        \n",
    "def create_testing_yaml(test_csv, i):\n",
    "    ''' Make a yaml file for prediction. The base of it is presented above.\n",
    "    '''\n",
    "    \n",
    "    model_name = split_names[i].split('.')[0] + '.pth'\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    test_file: {}\n",
    "    model: {}\n",
    "    \n",
    "# TESTING SETTINGS\n",
    "    threshold: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}  \n",
    "    '''.format(test_csv,\n",
    "               model_name,\n",
    "               threshold,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = test_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "    \n",
    "\n",
    "def create_training_yaml(train_csv, val_csv, i):\n",
    "    ''' Make a yaml file for training. The base of it is presented above.\n",
    "    '''\n",
    "    yaml_str = '''\\\n",
    "# INITIAL SETTINGS\n",
    "    train_file: {}\n",
    "    val_file: {}\n",
    "\n",
    "# TRAINING SETTINGS\n",
    "    batch_size: {}\n",
    "    num_workers: {}\n",
    "    epochs: {}\n",
    "    lr: {:f}\n",
    "    weight_decay: {:f}\n",
    "\n",
    "# DEVICE CONFIGS\n",
    "    device_count: {}   \n",
    "    '''.format(train_csv,\n",
    "               val_csv,\n",
    "               batch_size,\n",
    "               num_workers, \n",
    "               epochs,\n",
    "               lr,\n",
    "               weight_decay,\n",
    "               device_count)\n",
    "    \n",
    "    yaml_path = train_yaml_save_path\n",
    "    save_yaml(yaml_str, yaml_path, i)\n",
    "\n",
    "\n",
    "# Make the yaml files\n",
    "for i, data in enumerate(tvt_combs):\n",
    "    train, val, test = data\n",
    "\n",
    "    # Find the related train csv\n",
    "    train_csvs = sorted([os.path.splitext(db)[0] for db in train])\n",
    "    train_csv = '_'.join(sorted(train_csvs, key=str.lower)) + '.csv'\n",
    "\n",
    "    assert os.path.join(os.path.join(csv_path, train_csv)), 'CanÂ´t find the related train csv file.'\n",
    "    \n",
    "    print('Training data: `{}`, validation data: `{}`, test data: `{}`'.format(train_csv, val, test))\n",
    "    \n",
    "    create_training_yaml(train_csv, val, i)\n",
    "    create_testing_yaml(test, i)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4622c35",
   "metadata": {},
   "source": [
    "Now all the yaml files for training, validation and testing are created! The training yaml files are located in `/configs/training/train_dbwise_smoke/` and the testing yaml files in `/configs/predicting/predict_dbwise_smoke/`. There are also the combined csv files for ECGs created in `/data/split_csvs/dbwise_smoke/`.\n",
    "\n",
    "<font color=red>**NOTE 1!**</font> It is extremely important that in the testing yaml file the model is set with the same name as the yaml file which the model is trained with. E.g. when a model is trained using `split_1.yaml`, it will be saved as `split_1.pth`. This makes using the repository much easier and simpler. Mind this, if you want to edit the code below.\n",
    "\n",
    "<font color=red>**NOTE 2!**</font> If you are now wondering why the yaml files don't have the csv files in single quotation marks, it's ok. Scripts are able to read and load the values from the yaml files even without those marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "af433470baea3cbfb1d2a9219a544bb72a17c8a5091280fdb93be39946c5da4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
