{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7df4c",
   "metadata": {},
   "source": [
    "# <font color = teal> Introduction to data handling </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23f477ab",
   "metadata": {},
   "source": [
    "This notebook contains information about\n",
    "\n",
    "1)  how to download data into the repository (especially the Physionet 2021 data)\n",
    "\n",
    "2)  label mapping when ECGs are labels with different diagnostic codes\n",
    "\n",
    "3)  how to preprocess data if needed\n",
    "\n",
    "4)  the base idea of splitting data into csv files and how to perform it\n",
    "\n",
    "When you have performed possible preprocessing and the data splitting into csv files, you may want to create `yaml` files based on these files for training and testing. To do this, check the notebooks [Yaml files of database-wise split for training and testing](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of stratified split for training and testing](2_physionet_stratified_yaml_files.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e730",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 1) Downloading data </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13bf81a5",
   "metadata": {},
   "source": [
    "### <font color = teal> PhysioNet Challenge 2021 </font>\n",
    "\n",
    "The exploration of the dataset is available in the notebook [Exploration of the PhysioNet2021 data](exploration_physionet2021_data.ipynb).\n",
    "\n",
    "There are two ways to download the PhysioNet Challenge 2021 data in `tar.gz` format: \n",
    "\n",
    "1) Download the data manually from [here](https://moody-challenge.physionet.org/2021/) under **Data Access**\n",
    "\n",
    "2) Let this notebook do the job with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21446148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os, re\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1d7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Download the tar.gz files of the PhysioNet2021 data\n",
    "\n",
    "!wget -O WFDB_CPSC2018.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_CPSC2018_2.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_StPetersburg.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTB.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTBXL.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ga.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_ChapmanShaoxing.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ningbo.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0fdc",
   "metadata": {},
   "source": [
    "Once the `tar.gz` files are downloaded, they need to be extracted to the `data` directory which is located in the root of the repository. The files may be needed to be extracted based on the source database as follows:\n",
    "\n",
    "- CPSC Database and CPSC-Extra Database\n",
    "- St. Petersberg (INCART) Database\n",
    "- PTB and PTB-XL Database\n",
    "- The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "- Chapman-Shaoxing and Ningbo Database\n",
    "\n",
    "Let's first get the names of the `tar.gz` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WFDB_CPSC2018.tar.gz\n",
      "1 WFDB_CPSC2018_2.tar.gz\n",
      "2 WFDB_ChapmanShaoxing.tar.gz\n",
      "3 WFDB_Ga.tar.gz\n",
      "4 WFDB_Ningbo.tar.gz\n",
      "5 WFDB_PTB.tar.gz\n",
      "6 WFDB_PTBXL.tar.gz\n",
      "7 WFDB_StPetersburg.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# All tar.gz files (in the current working directory)\n",
    "curr_path = os.getcwd()\n",
    "targz_files = [file for file in os.listdir(curr_path) if os.path.isfile(os.path.join(curr_path, file)) and file.endswith('tar.gz') and file.startswith('WFDB')]\n",
    "\n",
    "# Let's sort the files\n",
    "targz_files = sorted(targz_files)\n",
    "\n",
    "for i, file in enumerate(targz_files):\n",
    "    print(i, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560e002",
   "metadata": {},
   "source": [
    "So the `tar.gz` files listed above will be extracted as follows:\n",
    "\n",
    "* WFDB_CPSC2018.tar.gz + WFDB_CPSC2018_2.tar.gz\n",
    "* WFDB_StPetersburg.tar.gz\n",
    "* WFDB_PTB.tar.gz + WFDB_PTBXL.tar.gz\n",
    "* WFDB_Ga.tar.gz\n",
    "* WFDB_ChapmanShaoxing.tar.gz + WFDB_Ningbo.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz')\n",
      "('WFDB_StPetersburg.tar.gz',)\n",
      "('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz')\n",
      "('WFDB_Ga.tar.gz',)\n",
      "('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "# Let's make the split as tuples of tar.gz files\n",
    "# NB! If the split mentioned above wanted, SORTING is really important!\n",
    "tar_split = [(targz_files[0], targz_files[1]),\n",
    "             (targz_files[7], ),\n",
    "             (targz_files[5], targz_files[6]),\n",
    "             (targz_files[3], ),\n",
    "             (targz_files[2], targz_files[4])]\n",
    "\n",
    "print(*tar_split, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f886269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract files from a given tar to a given directory\n",
    "# Will exclude subdirectories from a given tar and load all the files directly to the given directory\n",
    "def extract_files(tar, directory):\n",
    "    \n",
    "    file = tarfile.open(tar, 'r')\n",
    "    \n",
    "    n_files = 0\n",
    "    for member in file.getmembers():\n",
    "        if member.isreg(): # Skip if the TarInfo is not file\n",
    "            member.name = os.path.basename(member.name) # Reset path\n",
    "            file.extract(member, directory)\n",
    "            n_files += 1\n",
    "    \n",
    "    file.close() \n",
    "    re_dir = re.search('data.*', directory)[0]\n",
    "    print('- {} files extracted to {}'.format(n_files, './'+re_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e560b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tar_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m dir_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mCPSC_CPSC-Extra\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mINCART\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPTB_PTBXL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG12EC\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mChapmanShaoxing_Ningbo\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[39m# Extracting right files to right subdirectories\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfor\u001b[39;00m tar, directory \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tar_split, dir_names):\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mExtracting tar.gz file(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to the \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m directory\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(tar, directory))\n\u001b[0;32m     19\u001b[0m     \u001b[39m# Saving path for the specific files\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tar_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# Path to the physionet_data directory, i.e., save the dataset here\n",
    "data_path = os.path.join(abs_path.parent.absolute(), 'data', 'physionet_data')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Directories to which extract the data\n",
    "# NB! Gotta be at the same length than 'tar_split'\n",
    "dir_names = ['CPSC_CPSC-Extra', 'INCART', 'PTB_PTBXL', 'G12EC', 'ChapmanShaoxing_Ningbo']\n",
    "\n",
    "# Extracting right files to right subdirectories\n",
    "for tar, directory in zip(tar_split, dir_names):\n",
    "    \n",
    "    print('Extracting tar.gz file(s) {} to the {} directory'.format(tar, directory))\n",
    "    \n",
    "    # Saving path for the specific files\n",
    "    save_tmp = os.path.join(data_path, directory)\n",
    "    # Preparing the directory\n",
    "    if not os.path.exists(save_tmp):\n",
    "        os.makedirs(save_tmp)\n",
    "        \n",
    "    if len(tar) > 1: # More than one database in tuple\n",
    "        for one_tar in tar:\n",
    "            extract_files(one_tar, save_tmp)\n",
    "    else: # Only one database in tuple\n",
    "        extract_files(tar[0], save_tmp)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b35f9",
   "metadata": {},
   "source": [
    "Now total of **176Â 506** files (if we want to believe the data exploration presented above) should be located in the `physionet_data` directory as one ECG recording consists of a binary MATLAB v4 file and a text file in header format. For a double check, the number of files can be easily counted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 176506 files\n"
     ]
    }
   ],
   "source": [
    "total_files = 0\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    total_files += len(files)\n",
    "    \n",
    "print('Total of {} files'.format(total_files))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec35b911",
   "metadata": {},
   "source": [
    "### <font color = teal> Getting data from other sources </font>\n",
    "\n",
    "New data can be downloaded and used with this repository when few quidelines are followed:\n",
    "\n",
    "1\\) `MATLAB v4` (.mat) and `h5` formats are supported for ECG data. During setting up training or testing, ECGs are stored into `torch.utils.data.Dataset` using the following fuction in the `dataset_utils.py` script in `src/dataloader/`:\n",
    "\n",
    "```\n",
    "def load_data(case):\n",
    "    ''' Load a MATLAB v4 file or a H5 file of an ECG recording\n",
    "    '''\n",
    "\n",
    "    if case.endswith('.mat'):\n",
    "        x = loadmat(case)\n",
    "        return np.asarray(x['val'], dtype=np.float64)\n",
    "    else:\n",
    "        with h5py.File(case) as f:\n",
    "            x = f['ecg'][()]\n",
    "        return np.asarray(x, dtype=np.float64)\n",
    "```\n",
    "\n",
    "So, there is either a `val` column in the `MATLAB` file or a `ecg` column in the `H5` file.\n",
    "\n",
    "2\\) Metadata such as diagnoses, age and gender are loaded from either in `WFDB header format` format (.hea) or from csv files. Also other information about ECGs, e.g. sample frequency, is stored in them. Such files are needed when creating csv files of ECG samples with the `create_data_csvs.py` script. Header files have structure like one below:\n",
    "\n",
    "```\n",
    "JS00001 12 500 5000 23-Mar-2021 20:20:47\n",
    "JS00001.mat 16+24 1000/mV 16 0 -254 21756 0 I\n",
    "JS00001.mat 16+24 1000/mV 16 0 264 -599 0 II\n",
    "JS00001.mat 16+24 1000/mV 16 0 517 -22376 0 III\n",
    "JS00001.mat 16+24 1000/mV 16 0 -5 28232 0 aVR\n",
    "JS00001.mat 16+24 1000/mV 16 0 -386 16619 0 aVL\n",
    "JS00001.mat 16+24 1000/mV 16 0 390 15121 0 aVF\n",
    "JS00001.mat 16+24 1000/mV 16 0 -98 1568 0 V1\n",
    "JS00001.mat 16+24 1000/mV 16 0 -312 -32761 0 V2\n",
    "JS00001.mat 16+24 1000/mV 16 0 -98 32715 0 V3\n",
    "JS00001.mat 16+24 1000/mV 16 0 810 15193 0 V4\n",
    "JS00001.mat 16+24 1000/mV 16 0 810 14081 0 V5\n",
    "JS00001.mat 16+24 1000/mV 16 0 527 32579 0 V6\n",
    "#Age: 85\n",
    "#Sex: Male\n",
    "#Dx: 164889003,59118001,164934002\n",
    "#Rx: Unknown\n",
    "#Hx: Unknown\n",
    "#Sx: Unknown\n",
    "```\n",
    "The third value in the first row is the sample frequency, and age, gender and diagnoses are gotten from the lines 14-16. The 12 lines after the first one are corresponding the 12 leads of the ECG recordings, `Rx` is refering to medical prespriction, `Hx` to the medical history and `Sx` to symptom or surgery. There are similar columns in the csv files: age as `Age`, gender as `Sex` and diagnoses in SNOMED CT Codes as `SNOMEDCTCode`. Note that whether the metadata is in a csv file or in a header file, <font color = red><i>all metadata files should be located in the same directory than corresponding ECGs are located</i></font>.\n",
    "\n",
    "3\\) The easiest way to handle the repository is to download data into the `data` directory. There are some initialized paths to point to the mentioned directory, for example when creating the csv files of data or the yaml files for configurations of training and testing.\n",
    "\n",
    "The above code extracts tar.gz files and the chunk consisting of `extract_files(tar, directory)` is generally usable. The function parameters `tar` refers to tar.gz file which needs to be extracted, and `save_path` refers to the path in which the file is extracted to. The path is formatted as an absolute path. For example, the following code can be used for such purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edd6be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other sources\n",
    "## -------------\n",
    "'''\n",
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# The name of the tar gz file (located in the current directory)\n",
    "tar = 'records.tar.gz'\n",
    "save_path = os.path.join(abs_path.parent.absolute(), 'data', 'Shandong')\n",
    "#extract_files(tar, save_path)\n",
    "\n",
    "# If needed, the samples can be renamed\n",
    "samples = sorted(os.listdir(save_path))  # get the current names of the samples\n",
    "path_samples = sorted([os.path.join(save_path, s) for s in samples]) # add path to the current names\n",
    "new_names = [name.replace('A', 'SPH') for name in samples] # rename the beginning of the file (e.g., A0001.h5 to SPH0001.h5)\n",
    "path_new_names = sorted([os.path.join(save_path, nn) for nn in new_names]) # add path the the old names\n",
    "\n",
    "# Rename samples\n",
    "for old, new in zip(path_samples, path_new_names):\n",
    "    os.rename(old, new)\n",
    "\n",
    "# Also, if csv file of metadata has the IDs too, change them if needed\n",
    "csv_file = pd.read_csv('metadata.csv')\n",
    "csv_file ['ECG_ID'] = [s.replace('.h5', '') for s in new_names]\n",
    "\n",
    "# Be also sure that we have a sample frequency in it\n",
    "csv_file['fs'] = 500\n",
    "csv_file.to_csv(os.path.join(save_path, 'metadata.csv'), index=False)\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d92f3c0",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## <font color = teal>2) Label mapping </font>\n",
    "\n",
    "The main diagnostic code system used in this repository is SNOMED CT Codes.\n",
    "\n",
    "As ECGs can labeled with different codes, the `label_mapping.py` script is provided to convert codes that are not SNOMED CT Codes. <i>The assumption is that the metadata of spesific data set is found from a csv file</i>. The main idea of the script is that it maps the labels found from the metadata file using `AHA_SNOMED_mapping.csv` (found in the `data` directory) and adds corresponding SNOMED CT Code in the additional `SNOMEDCTCodes` column. The rest of the metadata file remains unchanged. The `AHA_SNOMED_mapping.csv` file contains some of the diagnostic statements conforming to the AHA standard and their corresponding SNOMED CT Codes in the following form:\n",
    "\n",
    "Rhythm|AHA_Code|SNOMEDCTCode \n",
    "------|---------|-------------\n",
    "Sinus Rhythm|1|426783006 \n",
    "Atrial Fibrillation|50|164889003\n",
    "Atrial Fibrillation |346|164889003 \n",
    "Atrial Fibrillation |347|164889003\n",
    "Atrial Flutter|51|164890007\n",
    "Premature Atrial complexes(conducting and non-conducting)|30|284470004\n",
    "... | ... | ...\n",
    "\n",
    "As long as the structure of the csv file is not violated, new codes can be added. The `Rhythm` column is not used, but it helps to keep track with the diagnoses. The metadata csv file will be replaced with the updated one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f2cd392",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## <font color = teal> 3) Preprocessing data (optional) </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23022168",
   "metadata": {},
   "source": [
    "All the data can be preprocessed with different transforms with the `preprocess_data.py` script. There are two important attributes to consider:\n",
    "\n",
    "```\n",
    "# Original data location\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'smoke_data')\n",
    "\n",
    "# New location for preprocessed data\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_smoke_data')\n",
    "```\n",
    "\n",
    "`from_directory` refers to the directory where the data in the original format is loaded from, such as the downloaded Physionet Challenge 2021 data. `new_directory` refers to the new location to where the preprocessed ECGs are saved. Note that <i> whenever the preprocessed ECGs are saved, the metadata files (e.g. a csv file or each corresponding hea file) needs to be copied to that location</i>. The script will do this too.\n",
    "\n",
    "By default there are two transforms used, a band-pass filter and linear interpolation:\n",
    "\n",
    "```\n",
    "# ------------------------------\n",
    "# --- PREPROCESS TRANSFORMS ----\n",
    "\n",
    "# - BandPass filter \n",
    "bpf = BandPassFilter(fs = ecg_fs)\n",
    "ecg = bpf(ecg)\n",
    "\n",
    "# - Linear interpolation\n",
    "linear_interp = Linear_interpolation(fs_new = 257, fs_old = ecg_fs)\n",
    "ecg = linear_interp(ecg)\n",
    "\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "```\n",
    "\n",
    "The preprocessing part **is not mandatory for the repository to work**, but if transforms, such as the two mentioned, are used e.g. during the training phase, that can significantly slow down training. That's why it's recommended to preprocess the data before training using the script mentioned.\n",
    "\n",
    "All the other transforms are set in the `dataset.py` script in `src/dataloader/`, which is run during training. Several transforms are already available in the script `transforms.py` --- from where `Linear_interpolation` and `BandPassFilter` can be found too --- in the same path.\n",
    "\n",
    "### <font color = teal> Terminal command </font>\n",
    "\n",
    "To use the script, simply use the following command\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!** The preprocessed ECGs will have different names as the original ones so it's important to mind if the preprosessing part is done or not!</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e0d492",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 4) Splitting data into csv files </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f439d39e",
   "metadata": {},
   "source": [
    "All the data splitting is done with the `create_data_csvs.py` script. The main idea for that script is to split the data into csv files which can be later used in training and testing.\n",
    "\n",
    "Csv files have the columns `path` (path for a spesific ECG recording), `age`, `gender` and all the diagnoses in SNOMED CT codes used as labels in classification. A value of 1 means that the patient has the disease. The main structure of csv files is as follows:\n",
    "\n",
    "\n",
    "| path  | age  | gender  | 10370003  | 111975006 | 164890007 | *other diagnoses...* |\n",
    "| ------------- |-------------|-------------| ------------- |-------------|-------------|-------------|\n",
    "| ./data/A0002.mat | 49.0 | Female | 0 | 0 | 1 | ... |\n",
    "| ./data/A0003.mat | 81.0 | Female | 0 | 1 | 1 | ... |\n",
    "| ./data/A0004.mat | 45.0 |  Male  | 1 | 0 | 0 | ... |\n",
    "| ... | ... |  ...  | ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "The script includes several attributes which need to be considered in the main block before running: \n",
    "\n",
    "1) The split itself is needed to be spesified using the `stratified` attribute which is a boolean. If the attribute is set `True`, the script performs stratified data split and respectively, if `False`, the database-wise split is performed. \n",
    "\n",
    "2) The `data_dir` attribute should be set to point to the right data directory where the data is loaded from. By default it's set to load the data from the `physionet_preprocessed_smoke` directory, which is the subdirectory of the `data` directory. \n",
    "\n",
    "3) The `csv_dir` attribute should be set to point to the wanted directory where the created csv files will be saved. By default it's set to save the csv files to the `stratified_smoke` directory which is found from `../data/split_csvs`.\n",
    "\n",
    "4) The class labels are needed to be set with the `labels` attribute in the script. By default the labels are \n",
    "`'426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009' and '270492004'`, which are ten most common labels found in the [Exploration of the PhysioNet 2021 Data](./exploration_physionet2021_data.ipynb). The numbers of each diagnosis are represented below:\n",
    "\n",
    "name | SNOMED CT code | Total number of diagnoses<br>in the whole data\n",
    "-----|----------------|-------------------------------------------\n",
    "sinus rhythm |426783006 | 28971\n",
    "sinus bradycardia| 426177001 | 18918 \n",
    "t wave abnormal| 164934002 | 11716\n",
    "sinus tachycardia |427084000 | 9657 \n",
    "atrial flutter| 164890007 | 8374\n",
    "left axis deviation |39732003 | 7631 \n",
    "atrial fibrillation |164889003 | 5255 \n",
    "t wave inversion| 59931005 | 3989 \n",
    "sinus arrhythmia |427393009 | 3790\n",
    "1st degree av block| 270492004 | 3534 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c76a439",
   "metadata": {},
   "source": [
    "The splitting itself can be done in two ways:\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. Above, the data was extracted in the following way \n",
    "\n",
    "   * CPSC Database and CPSC-Extra Database\n",
    "   * St. Petersberg (INCART) Database\n",
    "   * PTB and PTB-XL Database\n",
    "   * The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "   * Chapman-Shaoxing and Ningbo Database\n",
    "   \n",
    "This structure can be used as a baseline for the data split. Simply, the function `dbwise_csvs(data_directory, save_directory, labels)` uses this structure and creates csv files based on it. The `data_directory` parameter refers to the location of the data (note that subdirectories are considered to be different databases), `save_directory` refers to the location where the csv files will be saved, and `labels` refers to the list of Snomed CT Coded labels which will be used in classification. Csv files are named according to the directories from which they were created, e.g., a csv file of CPSC Database and CPSC-Extra Database is names as `CPSC_CPSC-Extra.csv`.\n",
    "\n",
    "As models read only one csv file from which it gets the paths of the ECGs during training and testing, there might be need to construct multiple databases into one csv file, e.g. if CPSC-Extra, CPSC, G12EC, PTB amd PTB XL are used for training. These multiple combinations of different databases are created in the script but there's an assumption behind the split: As the `data_directory` parameter is given, from where the names of the databases (a.k.a subdirectories) are read, <i>one is considered as a test set, one as a validation set and all the others as a training set</i>.\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>. The function `stratified_csvs(data_directory, save_directory, labels, train_test_splits)` will perform the stratified split. The parameters are similar to the ones with the function `dbwise_csvs` but there is also the `train_test_splits` parameter which refers to the dictionary of train-test splits. The dictionary is a nested dictionary, i.e. a collection of dictionaries, where the internal directories refer to spesific train-test splits. For example, by default there's one train-test split set in the `train_test_splits` dictionary as follows:\n",
    "\n",
    "   ```\n",
    "   train_test_splits = {\n",
    "   'split_1': {    \n",
    "         'train': ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "         'test': 'CPSC_CPSC-Extra'\n",
    "      }\n",
    "   }\n",
    "   ```\n",
    "where `split_1` is simply a name for this particular split, and it has keys `train` and `test` to initialize which databases are seen as training data and which ones as test data. Training data is further divided into training and validation sets. Names (e.g. `split_1`) are used to name the csv files.\n",
    "\n",
    "Stratification itself is performed by the multilabel cross validator `MultilabelStratifiedShuffleSplit(n_splits, test_size, train_size, random_state)` from `iterative-stratification` package. The script will be using n_splits sized of the length of training dataset (in the yaml file it will be *4* as data is gathered from 'G12EC', 'INCART', 'PTB_PTBXL' and 'ChapmanShaoxing_Ningbo'). *n_splits must always be at least 2!* More information about this and other multilabel cross validators is available in [the GitHub repository of iterative-stratification](https://github.com/trent-b/iterative-stratification)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db16fe9b",
   "metadata": {},
   "source": [
    "### <font color = teal> About the naming of csv files </font>\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. The csv files of the database-wise split are quite self-explanatory: The csv files are named after the database from where the data is, for example, `PTB_PTBXL.csv`. The combinated csv files (which are created while making the yaml files in the notebook [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb)) are named after the combination of the databases from which the csv file is structured. For example, if the training data is from the databases CPSC/CPSC-Extra, INCART, and PTB/PTB-XL Databases, the combined csv files will be named as `CPSC_CPSC-Extra_INCART_PTB_PTBXL.csv`.\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>. As there are 5 different data sources, there are 5 different data splits to be made out of them, i.e., in each split, one spesific dataset is used as testing set and all the others as training set. The `create_data_csvs.py` script will name the resulting csv files using information from the keys of the `train_test_splits` dictionary and from the results of the `MultilabelStratifiedShuffleSplit()` cross validator. For example, the csv names could be the following:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "train_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp val_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp test_split_1.csv\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "First, the csvs are separated from each other with `train`, `val` and `test`. Then, as the `train_test_splits` dictionary has keys indexing the splits (e.g. `split_1` and `split_1`), the first index refers to this indexing. The latter index refers to the results of the `MultilabelStratifiedShuffleSplit()` cross validator: As there are 4 different databases from which data is gathered and stratified, it results 4 different splits of training and validation set. So, the latter indexing is due to the functionality of the mentioned cross validator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0399870e",
   "metadata": {},
   "source": [
    "### <font color = teal>  Terminal commands </font>\n",
    "\n",
    "After initializing the needed attributes, the terminal command to perform wanted data split is the following one:\n",
    "\n",
    "```\n",
    "python create_data_csvs.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440b5d2",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## <font color = teal> Example: smoke testing </font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3484469",
   "metadata": {},
   "source": [
    "*All the data files for smoke testing are available in the repository.*\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_dir` **attribute is set with the assumption that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as the** `smoke_data` **directory.** The paths for ECGs will be different in the csv files depending on whether preprocessing has been used or not.</font>\n",
    "\n",
    "First, we want to **preprocess the data**. We make sure that the `preprocess_data.py` script has the original and new directories set as follows\n",
    "\n",
    "```\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'smoke_data')\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "The `from_directory` attribute refers to the directory where the original data is located, and the `new_directory` attribute where the preprocessed data is saved. Now preprocessing is performed with the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "When data is preprocessed, we can move on to **split the data into csv files**. Remember to check that the attributes are set as below before running the following command:\n",
    "\n",
    "```\n",
    "python create_data_csvs.py\n",
    "```\n",
    "\n",
    "###  <font color = forestgreen> Database-wise split </font>\n",
    "\n",
    "The `create_data_csvs.py` script should have the following attributes set **before the `if-else` statement** as follows:\n",
    "\n",
    "```\n",
    "stratified = False\n",
    "data_dir =  'preprocessed_smoke_data'\n",
    "csv_dir =  'dbwise_smoke'\n",
    "labels = ['426783006', '426177001', '427084000', '164890007', '164889003', '427393009']\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/dbwise_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "ChapmanShaoxing_Ningbo.csv\n",
    "CPSC_CPSC-Extra.csv\n",
    "G12EC.csv\n",
    "INCART.csv\n",
    "PTB_PTBXL.csv\n",
    "```\n",
    "\n",
    "### <font color = forestgreen> Stratified split </font>\n",
    "\n",
    "Stratified data split is performed using dictionary of dictionaries where the wanted train-test splits are set. There is one split which is made by running the file.\n",
    "\n",
    "- Train data is from the directories *G12EC, INCART, PTB_PTBXL* and *ChapmanShaoxing_Ningbo*\n",
    "- Test data is from the directory *CPSC_CPSC-Extra*.\n",
    "\n",
    "The following attributes set **before the `if-else` statement** as follows:\n",
    "\n",
    "```\n",
    "stratified = True\n",
    "data_dir =  'preprocessed_smoke_data'\n",
    "csv_dir =  'stratified_smoke'\n",
    "labels = ['426783006', '426177001', '427084000', '164890007', '164889003', '427393009']\n",
    "```\n",
    "\n",
    "And so specify, which databases are used as training set and which one(s) as testing set, the `train_test_splits` attribute should be set **in the if block**.\n",
    "\n",
    "```\n",
    "train_test_splits = {\n",
    "    'split_1': {    \n",
    "        'train': ['G12EC', 'Shandong', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "        'test': 'CPSC_CPSC-Extra'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/stratified_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "test_split1.csv\n",
    "train_split_1_1.csv\n",
    "train_split_1_2.csv\n",
    "train_split_1_3.csv\n",
    "train_split_1_4.csv\n",
    "val_split_1_1.csv\n",
    "val_split_1_2.csv\n",
    "val_split_1_3.csv\n",
    "val_split_1_4.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "af433470baea3cbfb1d2a9219a544bb72a17c8a5091280fdb93be39946c5da4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
