{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f7df4c",
   "metadata": {},
   "source": [
    "# <font color = teal> Introduction to data handling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f477ab",
   "metadata": {},
   "source": [
    "This notebook contains information about\n",
    "\n",
    "1)  how to download data into the repository (especially the Physionet 2021 data)\n",
    "\n",
    "2)  how to preprocess data if needed\n",
    "\n",
    "3)  the base idea of splitting data into csv files and how to perform it\n",
    "\n",
    "When you have performed possible preprocessing and the data splitting into csv files, you may want to create `yaml` files based on these files for training and testing. To do this, check the notebooks [Yaml files of database-wise split for training and testing](2_physionet_DBwise_yaml_files.ipynb) and [Yaml files of stratified split for training and testing](2_physionet_stratified_yaml_files.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624e730",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 1) Downloading data </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf81a5",
   "metadata": {},
   "source": [
    "### <font color = teal> PhysioNet Challenge 2021 </font>\n",
    "\n",
    "The exploration of the dataset is available in the notebook [Exploration of the PhysioNet2021 data](exploration_physionet2021_data.ipynb).\n",
    "\n",
    "There are two ways to download the Physionet Challenge 2021 data in `tar.gz` format: \n",
    "\n",
    "1) Download the data manually from [here](https://moody-challenge.physionet.org/2021/) under **Data Access**\n",
    "\n",
    "2) Let this notebook do the job with the following code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28e5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import os, re\n",
    "import tarfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d7b0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-25 12:36:48--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 827672464 (789M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018.tar.g 100%[===================>] 789.33M  22.0MB/s    in 60s     \n",
      "\n",
      "2023-01-25 12:37:49 (13.1 MB/s) - ‘WFDB_CPSC2018.tar.gz’ saved [827672464/827672464]\n",
      "\n",
      "--2023-01-25 12:37:51--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 423189282 (404M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_CPSC2018_2.tar.gz’\n",
      "\n",
      "WFDB_CPSC2018_2.tar 100%[===================>] 403.58M  10.4MB/s    in 55s     \n",
      "\n",
      "2023-01-25 12:38:46 (7.40 MB/s) - ‘WFDB_CPSC2018_2.tar.gz’ saved [423189282/423189282]\n",
      "\n",
      "--2023-01-25 12:38:47--  https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 591394709 (564M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_StPetersburg.tar.gz’\n",
      "\n",
      "WFDB_StPetersburg.t 100%[===================>] 564.00M  15.3MB/s    in 61s     \n",
      "\n",
      "2023-01-25 12:39:48 (9.30 MB/s) - ‘WFDB_StPetersburg.tar.gz’ saved [591394709/591394709]\n",
      "\n",
      "--2023-01-25 12:39:54--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 902791782 (861M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTB.tar.gz’\n",
      "\n",
      "WFDB_PTB.tar.gz     100%[===================>] 860.97M  13.5MB/s    in 59s     \n",
      "\n",
      "2023-01-25 12:40:54 (14.6 MB/s) - ‘WFDB_PTB.tar.gz’ saved [902791782/902791782]\n",
      "\n",
      "--2023-01-25 12:40:57--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1371121893 (1.3G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_PTBXL.tar.gz’\n",
      "\n",
      "WFDB_PTBXL.tar.gz   100%[===================>]   1.28G  9.33MB/s    in 1m 56s  \n",
      "\n",
      "2023-01-25 12:42:54 (11.3 MB/s) - ‘WFDB_PTBXL.tar.gz’ saved [1371121893/1371121893]\n",
      "\n",
      "--2023-01-25 12:42:54--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 502266323 (479M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ga.tar.gz’\n",
      "\n",
      "WFDB_Ga.tar.gz      100%[===================>] 479.00M  15.4MB/s    in 51s     \n",
      "\n",
      "2023-01-25 12:43:46 (9.36 MB/s) - ‘WFDB_Ga.tar.gz’ saved [502266323/502266323]\n",
      "\n",
      "--2023-01-25 12:43:47--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 566560497 (540M) [application/octet-stream]\n",
      "Saving to: ‘WFDB_ChapmanShaoxing.tar.gz’\n",
      "\n",
      "WFDB_ChapmanShaoxin 100%[===================>] 540.31M  6.53MB/s    in 46s     \n",
      "\n",
      "2023-01-25 12:44:34 (11.8 MB/s) - ‘WFDB_ChapmanShaoxing.tar.gz’ saved [566560497/566560497]\n",
      "\n",
      "--2023-01-25 12:44:34--  https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
      "Resolving pipelineapi.org (pipelineapi.org)... 35.237.166.166\n",
      "Connecting to pipelineapi.org (pipelineapi.org)|35.237.166.166|:9555... connected.\n",
      "HTTP request sent, awaiting response... 200 \n",
      "Length: 1889846085 (1.8G) [application/octet-stream]\n",
      "Saving to: ‘WFDB_Ningbo.tar.gz’\n",
      "\n",
      "WFDB_Ningbo.tar.gz  100%[===================>]   1.76G  22.2MB/s    in 2m 35s  \n",
      "\n",
      "2023-01-25 12:47:10 (11.6 MB/s) - ‘WFDB_Ningbo.tar.gz’ saved [1889846085/1889846085]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First we need the tar.gz files of each database so let's download them\n",
    "\n",
    "!wget -O WFDB_CPSC2018.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_CPSC2018_2.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_CPSC2018_2.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_StPetersburg.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining//WFDB_StPetersburg.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTB.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTB.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_PTBXL.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_PTBXL.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ga.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ga.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_ChapmanShaoxing.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_ChapmanShaoxing.tar.gz/\n",
    "        \n",
    "!wget -O WFDB_Ningbo.tar.gz \\\n",
    "https://pipelineapi.org:9555/api/download/physionettraining/WFDB_Ningbo.tar.gz/\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a0fdc",
   "metadata": {},
   "source": [
    "Once the `tar.gz` files are downloaded, they need to be extracted to the `data` directory which is located in the root of the repository. The files may be needed to be extracted based on the source database as follows:\n",
    "\n",
    "- CPSC Database and CPSC-Extra Database\n",
    "- St. Petersberg (INCART) Database\n",
    "- PTB and PTB-XL Database\n",
    "- The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "- Chapman-Shaoxing and Ningbo Database\n",
    "\n",
    "Let's first get the names of the `tar.gz` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b1e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 WFDB_CPSC2018.tar.gz\n",
      "1 WFDB_CPSC2018_2.tar.gz\n",
      "2 WFDB_ChapmanShaoxing.tar.gz\n",
      "3 WFDB_Ga.tar.gz\n",
      "4 WFDB_Ningbo.tar.gz\n",
      "5 WFDB_PTB.tar.gz\n",
      "6 WFDB_PTBXL.tar.gz\n",
      "7 WFDB_StPetersburg.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# All tar.gz files (in the current working directory)\n",
    "curr_path = os.getcwd()\n",
    "targz_files = [file for file in os.listdir(curr_path) if os.path.isfile(os.path.join(curr_path, file)) and file.endswith('tar.gz') and file.startswith('WFDB')]\n",
    "\n",
    "# Let's sort the files\n",
    "targz_files = sorted(targz_files)\n",
    "\n",
    "for i, file in enumerate(targz_files):\n",
    "    print(i, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560e002",
   "metadata": {},
   "source": [
    "So the `tar.gz` files listed above will be extracted as follows:\n",
    "\n",
    "* WFDB_CPSC2018.tar.gz + WFDB_CPSC2018_2.tar.gz\n",
    "* WFDB_StPetersburg.tar.gz\n",
    "* WFDB_PTB.tar.gz + WFDB_PTBXL.tar.gz\n",
    "* WFDB_Ga.tar.gz\n",
    "* WFDB_ChapmanShaoxing.tar.gz + WFDB_Ningbo.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131b041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz')\n",
      "('WFDB_StPetersburg.tar.gz',)\n",
      "('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz')\n",
      "('WFDB_Ga.tar.gz',)\n",
      "('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz')\n"
     ]
    }
   ],
   "source": [
    "# Let's make the split as tuples of tar.gz files\n",
    "# NB! If the split mentioned above wanted, SORTING is really important!\n",
    "tar_split = [(targz_files[0], targz_files[1]),\n",
    "             (targz_files[7], ),\n",
    "             (targz_files[5], targz_files[6]),\n",
    "             (targz_files[3], ),\n",
    "             (targz_files[2], targz_files[4])]\n",
    "\n",
    "print(*tar_split, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f886269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract files from a given tar to a given directory\n",
    "# Will exclude subdirectories from a given tar and load all the files directly to the given directory\n",
    "def extract_files(tar, directory):\n",
    "    \n",
    "    file = tarfile.open(tar, 'r')\n",
    "    \n",
    "    n_files = 0\n",
    "    for member in file.getmembers():\n",
    "        if member.isreg(): # Skip if the TarInfo is not file\n",
    "            member.name = os.path.basename(member.name) # Reset path\n",
    "            file.extract(member, directory)\n",
    "            n_files += 1\n",
    "    \n",
    "    file.close() \n",
    "    re_dir = re.search('data.*', directory)[0]\n",
    "    print('- {} files extracted to {}'.format(n_files, './'+re_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69e560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tar.gz file(s) ('WFDB_CPSC2018.tar.gz', 'WFDB_CPSC2018_2.tar.gz') to the CPSC_CPSC-Extra directory\n",
      "- 13754 files extracted to ./data/physionet_data/CPSC_CPSC-Extra\n",
      "- 6906 files extracted to ./data/physionet_data/CPSC_CPSC-Extra\n",
      "Extracting tar.gz file(s) ('WFDB_StPetersburg.tar.gz',) to the INCART directory\n",
      "- 148 files extracted to ./data/physionet_data/INCART\n",
      "Extracting tar.gz file(s) ('WFDB_PTB.tar.gz', 'WFDB_PTBXL.tar.gz') to the PTB_PTBXL directory\n",
      "- 1032 files extracted to ./data/physionet_data/PTB_PTBXL\n",
      "- 43674 files extracted to ./data/physionet_data/PTB_PTBXL\n",
      "Extracting tar.gz file(s) ('WFDB_Ga.tar.gz',) to the G12EC directory\n",
      "- 20688 files extracted to ./data/physionet_data/G12EC\n",
      "Extracting tar.gz file(s) ('WFDB_ChapmanShaoxing.tar.gz', 'WFDB_Ningbo.tar.gz') to the ChapmanShaoxing_Ningbo directory\n",
      "- 20494 files extracted to ./data/physionet_data/ChapmanShaoxing_Ningbo\n",
      "- 69810 files extracted to ./data/physionet_data/ChapmanShaoxing_Ningbo\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Absolute path of this file\n",
    "abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "# Path to the physionet_data directory, i.e., save the dataset here\n",
    "data_path = os.path.join(abs_path.parent.absolute(), 'data', 'physionet_data')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "# Directories to which extract the data\n",
    "# NB! Gotta be at the same length than 'tar_split'\n",
    "dir_names = ['CPSC_CPSC-Extra', 'INCART', 'PTB_PTBXL', 'G12EC', 'ChapmanShaoxing_Ningbo']\n",
    "\n",
    "# Extracting right files to right subdirectories\n",
    "for tar, directory in zip(tar_split, dir_names):\n",
    "    \n",
    "    print('Extracting tar.gz file(s) {} to the {} directory'.format(tar, directory))\n",
    "    \n",
    "    # Saving path for the specific files\n",
    "    save_tmp = os.path.join(data_path, directory)\n",
    "    # Preparing the directory\n",
    "    if not os.path.exists(save_tmp):\n",
    "        os.makedirs(save_tmp)\n",
    "        \n",
    "    if len(tar) > 1: # More than one database in tuple\n",
    "        for one_tar in tar:\n",
    "            extract_files(one_tar, save_tmp)\n",
    "    else: # Only one database in tuple\n",
    "        extract_files(tar[0], save_tmp)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b35f9",
   "metadata": {},
   "source": [
    "Now total of **176 506** files (if we want to believe the data exploration presented above) should be located in the `physionet_data` directory as one ECG recording consists of a binary MATLAB v4 file and a text file in header format. For a double check, the number of files can be easily counted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6cd287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 176506 files\n"
     ]
    }
   ],
   "source": [
    "total_files = 0\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    total_files += len(files)\n",
    "    \n",
    "print('Total of {} files'.format(total_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35b911",
   "metadata": {},
   "source": [
    "### <font color = teal> Other data sources\n",
    "\n",
    "Wanted data can also be downloaded from other sources when few quidelines are followed:\n",
    "\n",
    "1) When using this repository in training and testing, the model processes ECGs in `MATLAB v4` format (.mat) and header files in `WFDB header format` format (.hea). Header files consist of the describtion of the recording and patient attributes, including *diagnoses*. \n",
    "\n",
    "The following code is used to load the data from MATLAB files:\n",
    "\n",
    "```\n",
    "def load_data(case):\n",
    "    ''' Loading the MATLAB v4 file of ECG recording\n",
    "    '''\n",
    "    x = loadmat(case)\n",
    "    return np.asarray(x['val'], dtype=np.float64)\n",
    "```\n",
    "\n",
    "So there is a column named `val` in which the recording is located. This should be considered when loading other MATLAB files.\n",
    "\n",
    "2) Data should be located in the `data` directory. For example, when training and making predictions, the `data_root` attribute is set in `train_model.py` and `test_model.py` scripts to  indicate the path where the ECG recordings are loaded from.\n",
    "\n",
    "The above code extracts tar.gz files and the chunk consisting of `extract_files(tar, directory)` is generally usable. The function parameters `tar` refers to tar.gz file which needs to be extracted, and `save_path` refers to the path in which the file is extracted to. The path is formatted as an absolute path.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edd6be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tuhlei/digital_health_tech_files/git/12-lead-ecg-classifier/data/12-lead_ECG_AHA\n",
      "- 25770 files extracted to /home/tuhlei/digital_health_tech_files/git/12-lead-ecg-classifier/data/12-lead_ECG_AHA\n"
     ]
    }
   ],
   "source": [
    "## Other sources\n",
    "## -------------\n",
    "\n",
    "# Absolute path of this file\n",
    "#abs_path = Path(os.path.abspath(''))\n",
    "\n",
    "#tar = '12-lead_ECG_records.tar.gz'\n",
    "#save_path = os.path.join(abs_path.parent.absolute(), 'data', '12-lead_ECG_AHA')\n",
    "#print(save_path)\n",
    "#extract_files(tar, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2cd392",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## <font color = teal> 2) Preprocessing data (optional) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23022168",
   "metadata": {},
   "source": [
    "All the data can be preprocessed with different transforms with the `preprocess_data.py` script. There are two important attributes to consider:\n",
    "\n",
    "```\n",
    "# Original data location\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "\n",
    "# New location for preprocessed data\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "`from_directory` refers to the directory where the data in the original format is loaded from, such as the downloaded Physionet Challenge 2021 data. `new_directory` refers to the new location where the directory tree of the original data location is first copied using a function `copy_tree` from the module `distutils.dir_util`. After this, *each directory in the new location* is iterated over and all the ECGs (which should be in a MATLAB format) are preprocessed with wanted transforms. *An original version of ECG is afterwards deleted and the preprocessed one saved in the directory.*\n",
    "\n",
    "By default there are two transforms used, a band-pass filter and linear interpolation:\n",
    "\n",
    "```\n",
    "# ------------------------------\n",
    "# --- PREPROCESS TRANSFORMS ----\n",
    "\n",
    "# - BandPass filter \n",
    "bpf = BandPassFilter(fs = ecg_fs)\n",
    "ecg = bpf(ecg)\n",
    "\n",
    "# - Linear interpolation\n",
    "linear_interp = Linear_interpolation(fs_new = 257, fs_old = ecg_fs)\n",
    "ecg = linear_interp(ecg)\n",
    "\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "```\n",
    "\n",
    "The preprocessing part **is not mandatory for the repository to work**, but if transforms, such as the two mentioned, are used e.g. during the training phase, that can significantly slow down training. That's why it's recommended to preprocess the data before training using the script mentioned.\n",
    "\n",
    "All the other transforms are set in the `dataset.py` script in `src/dataloader/`, which is run during training. Several transforms are already available in the script `transforms.py` --- from where `Linear_interpolation` and `BandPassFilter` can be found too --- in the same path.\n",
    "\n",
    "### <font color = teal> Terminal command </font>\n",
    "\n",
    "To use the script, simply use the following command\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "<font color = red>**NOTE!** The preprocessed ECGs will have different names as the original ones so it's important to mind if the preprosessing part is done or not!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e0d492",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## <font color = teal> 3) Splitting data into csv files </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f439d39e",
   "metadata": {},
   "source": [
    "All the data splitting is done with the `create_data_split_csvs.py` script. The main idea for that script is to split the data into csv files which can be later used in training and testing.\n",
    "\n",
    "Csv files have the columns `path` (path for a spesific ECG recording), `age`, `gender` and all the diagnoses in SNOMED CT codes used as labels in classification. A value of 1 means that the patient has the disease. The main structure of csv files is as follows:\n",
    "\n",
    "\n",
    "| path  | age  | gender  | 10370003  | 111975006 | 164890007 | *other diagnoses...* |\n",
    "| ------------- |-------------|-------------| ------------- |-------------|-------------|-------------|\n",
    "| ./data/A0002.mat | 49.0 | Female | 0 | 0 | 1 | ... |\n",
    "| ./data/A0003.mat | 81.0 | Female | 0 | 1 | 1 | ... |\n",
    "| ./data/A0004.mat | 45.0 |  Male  | 1 | 0 | 0 | ... |\n",
    "| ... | ... |  ...  | ... | ... | ... | ... |\n",
    "\n",
    "\n",
    "The script includes several attributes which need to be considered in the main block before running: \n",
    "\n",
    "1) The split itself is needed to be spesified using the `stratified` attribute which is a boolean. If the attribute is set `True`, the script performs stratified data split and respectively, if `False`, the database-wise split is performed. \n",
    "\n",
    "2) The `data_dir` attribute should be set to point to the right data directory where the data is loaded from. By default it's set to load the data from the `physionet_preprocessed_smoke` directory, which is the subdirectory of the `data` directory. \n",
    "\n",
    "3) The `csv_dir` attribute should be set to point to the wanted directory where the created csv files will be saved. By default it's set to save the csv files to the `physionet_stratified_smoke` directory which is found from `../data/split_csvs`.\n",
    "\n",
    "4) The class labels are needed to be set with the `labels` attribute in the script. By default the labels are \n",
    "`'426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009' and '270492004'`, which are ten most common labels found in the [Exploration of the PhysioNet 2021 Data](./exploration_physionet2021_data.ipynb). The numbers of each diagnosis are represented below:\n",
    "\n",
    "name | SNOMED CT code | Total number of diagnoses<br>in the whole data\n",
    "-----|----------------|-------------------------------------------\n",
    "sinus rhythm |426783006 | 28971\n",
    "sinus bradycardia| 426177001 | 18918 \n",
    "t wave abnormal| 164934002 | 11716\n",
    "sinus tachycardia |427084000 | 9657 \n",
    "atrial flutter| 164890007 | 8374\n",
    "left axis deviation |39732003 | 7631 \n",
    "atrial fibrillation |164889003 | 5255 \n",
    "t wave inversion| 59931005 | 3989 \n",
    "sinus arrhythmia |427393009 | 3790\n",
    "1st degree av block| 270492004 | 3534 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76a439",
   "metadata": {},
   "source": [
    "The splitting itself can be done in two ways:\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. Above, the data was extracted in the following way \n",
    "\n",
    "   * CPSC Database and CPSC-Extra Database\n",
    "   * St. Petersberg (INCART) Database\n",
    "   * PTB and PTB-XL Database\n",
    "   * The Georgia 12-lead ECG Challenge (G12EC) Database\n",
    "   * Chapman-Shaoxing and Ningbo Database\n",
    "   \n",
    "This structure can be used as a baseline for the data split. Simply, the function `dbwise_csvs(data_directory, save_directory, labels)` uses this structure and creates csvs based on it. The `data_directory` parameter refers to the location of the data, `save_directory` refers to the location where the csv files will be saved, and `labels` refers to the list of Snomed CT Coded labels which will be used in classification. Csv files are named according to the directories from which they were created, e.g., a csv file of CPSC Database and CPSC-Extra Database is names as `CPSC_CPSC-Extra.csv`.\n",
    "\n",
    "We can use this structure when creating yaml files for training and testing. But for example if we need to train a model using the first four sources in the list and using only the Chapman-Shaoxing and Ningbo database in testing, we need to create combined yaml files for training phase. In training we only give one csv file for a model to read which ECGs to use. The other csv files, in which there are ECGs from different databases, are made in the notebook [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb) when the training and testing csv files are created.\n",
    "\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>. The function `stratified_csvs(data_directory, save_directory, labels, train_test_splits)` will perform the stratified split. The parameters are similar to the ones with the function `dbwise_csvs` but there is also the `train_test_splits` parameter which refers to the dictionary of train-test splits. The dictionary is a nested dictionary, i.e. a collection of dictionaries, where the internal directories refer to spesific train-test splits. For example, by default there's one train-test split set in the `train_test_splits` dictionary as follows:\n",
    "\n",
    "   ```\n",
    "   train_test_splits = {\n",
    "   'split_1': {    \n",
    "         'train': ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "         'test': 'CPSC_CPSC-Extra'\n",
    "      }\n",
    "   }\n",
    "   ```\n",
    "where `split_1` is simply a name for this particular split, and it has keys `train` and `test` to initialize which databases are seen as training data and which ones as test data. Training data is further divided into training and validation sets. Names (e.g. `split_1`) are used to name the csv files.\n",
    "\n",
    "Stratification itself is performed by the multilabel cross validator `MultilabelStratifiedShuffleSplit(n_splits, test_size, train_size, random_state)` from `iterative-stratification` package. The script will be using n_splits sized of the length of training dataset (in the yaml file it will be *4* as data is gathered from 'G12EC', 'INCART', 'PTB_PTBXL' and 'ChapmanShaoxing_Ningbo'). *n_splits must always be at least 2!* More information about this and other multilabel cross validators is available in [the GitHub repository of iterative-stratification](https://github.com/trent-b/iterative-stratification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db16fe9b",
   "metadata": {},
   "source": [
    "### <font color = teal> About the naming of csv files </font>\n",
    "\n",
    "<font color = forestgreen><b>Database-wise</b></font>. The csv files of the database-wise split are quite self-explanatory: The csv files are named after the database from where the data is, for example, `PTB_PTBXL.csv`. The combinated csv files (which are created while making the yaml files in the notebook [Yaml files of Database-wise Split for Training and Prediction](2_physionet_DBwise_yaml_files.ipynb)) are named after the combination of the databases from which the csv file is structured. For example, if the training data is from the databases CPSC/CPSC-Extra, INCART, and PTB/PTB-XL Databases, the combined csv files will be named as `CPSC_CPSC-Extra_INCART_PTB_PTBXL.csv`.\n",
    "\n",
    "<font color = forestgreen><b>Stratified</b></font>. As there are 5 different data sources, there are 5 different data splits to be made out of them, i.e., in each split, one spesific dataset is used as testing set and all the others as training set. The `create_data_split_csvs.py` script will name the resulting csv files using information from the keys of the `train_test_splits` dictionary and from the results of the `MultilabelStratifiedShuffleSplit()` cross validator. For example, the csv names could be the following:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "train_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp val_split_1_1.csv &nbsp&nbsp&nbsp&nbsp&nbsp test_split_1.csv\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "First, the csvs are separated from each other with `train`, `val` and `test`. Then, as the `train_test_splits` dictionary has keys indexing the splits (e.g. `split_1` and `split_1`), the first index refers to this indexing. The latter index refers to the results of the `MultilabelStratifiedShuffleSplit()` cross validator: As there are 4 different databases from which data is gathered and stratified, it results 4 different splits of training and validation set. So, the latter indexing is due to the functionality of the mentioned cross validator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399870e",
   "metadata": {},
   "source": [
    "### <font color = teal>  Terminal commands </font>\n",
    "\n",
    "After initializing the needed attributes, the terminal command to perform wanted data split is the following one:\n",
    "\n",
    "```\n",
    "python create_data_split_csvs.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440b5d2",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "## <font color = teal> Example: smoke testing </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3484469",
   "metadata": {},
   "source": [
    "*All the data files for smoke testing are available in the repository.*\n",
    "\n",
    "<font color = red>**NOTE!**</font> <font color = green> **Here, the** `data_dir` **attribute is set with the assumption that *the data is preprocessed*. If that's not the case, you should use, for example, the original data directory, such as the** `physionet_data_smoke` **directory.** The paths for ECGs will be different in the csv files depending on whether preprocessing has been used or not.</font>\n",
    "\n",
    "First, we want to **preprocess the data**. We make sure that the `preprocess_data.py` script has the original and new directories set as follows\n",
    "\n",
    "```\n",
    "from_directory = os.path.join(os.getcwd(), 'data', 'physionet_data_smoke')\n",
    "new_directory = os.path.join(os.getcwd(), 'data', 'physionet_preprocessed_smoke')\n",
    "```\n",
    "\n",
    "The `from_directory` attribute refers to the directory where the original data is located, and the `new_directory` attribute where the preprocessed data is saved. Now preprocessing is performed with the following command:\n",
    "\n",
    "```\n",
    "python preprocess_data.py\n",
    "```\n",
    "\n",
    "When data is preprocessed, we can move on to **split the data into csv files**. Remember to check that the attributes are set as below before running the following command:\n",
    "\n",
    "```\n",
    "python create_data_split_csvs.py\n",
    "```\n",
    "\n",
    "###  <font color = forestgreen> Database-wise split </font>\n",
    "\n",
    "The `create_data_split_csvs.py` script should have the following attributes set **before the `if-else` statement** as follows:\n",
    "\n",
    "```\n",
    "stratified = False\n",
    "data_dir =  'physionet_preprocessed_smoke'\n",
    "csv_dir =  'physionet_DBwise_smoke'\n",
    "labels = ['426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009', '270492004']\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/physionet_DBwise_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "ChapmanShaoxing_Ningbo.csv\n",
    "CPSC_CPSC-Extra.csv\n",
    "G12EC.csv\n",
    "INCART.csv\n",
    "PTB_PTBXL.csv\n",
    "```\n",
    "\n",
    "### <font color = forestgreen> Stratified split </font>\n",
    "\n",
    "Stratified data split is performed using dictionary of dictionaries where the wanted train-test splits are set. There is one split which is made by running the file.\n",
    "\n",
    "- Train data is from the directories *G12EC, INCART, PTB_PTBXL* and *ChapmanShaoxing_Ningbo*\n",
    "- Test data is from the directory *CPSC_CPSC-Extra*.\n",
    "\n",
    "The following attributes set **before the `if-else` statement** as follows:\n",
    "\n",
    "```\n",
    "stratified = True\n",
    "data_dir =  'physionet_preprocessed_smoke'\n",
    "csv_dir =  'physionet_stratified_smoke'\n",
    "labels = ['426783006', '426177001', '164934002', '427084000', '164890007', '39732003', '164889003', '59931005', '427393009', '270492004']\n",
    "```\n",
    "\n",
    "And so specify, which databases are used as training set and which one(s) as testing set, the `train_test_splits` attribute should be set **in the if block**.\n",
    "\n",
    "```\n",
    "train_test_splits = {\n",
    "    'split_1': {    \n",
    "        'train': ['G12EC', 'INCART', 'PTB_PTBXL', 'ChapmanShaoxing_Ningbo'],\n",
    "        'test': 'CPSC_CPSC-Extra'\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The csv files are saved in `./data/split_csvs/physionet_stratified_smoke/` where you will find the following files:\n",
    "\n",
    "```\n",
    "test_split1.csv\n",
    "train_split_1_1.csv\n",
    "train_split_1_2.csv\n",
    "train_split_1_3.csv\n",
    "train_split_1_4.csv\n",
    "val_split_1_1.csv\n",
    "val_split_1_2.csv\n",
    "val_split_1_3.csv\n",
    "val_split_1_4.csv\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "46963f0590c8313baefd57cb1336ee0094ee3a6da0b1eb974571013be2a14d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
